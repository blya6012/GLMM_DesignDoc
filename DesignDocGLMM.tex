\documentclass{article}

 \usepackage{url} 
\usepackage{amsthm,amsmath,amssymb,indentfirst,float}
\usepackage{verbatim}
\usepackage[sort,longnamesfirst]{natbib}
\newcommand{\pcite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\ncite}[1]{\citeauthor{#1}, \citeyear{#1}}
\DeclareMathOperator{\logit}{logit}
    \DeclareMathOperator{\var}{Var}
   %  \DeclareMathOperator{\det}{det}
     \DeclareMathOperator{\diag}{diag}

\usepackage{geometry}
%\geometry{hmargin=1.025in,vmargin={1.25in,2.5in},nohead,footskip=0.5in} 
%\geometry{hmargin=1.025in,vmargin={1.25in,0.75in},nohead,footskip=0.5in} 
%\geometry{hmargin=2.5cm,vmargin={2.5cm,2.5cm},nohead,footskip=0.5in}

\renewcommand{\baselinestretch}{1.25}

\usepackage{amsbsy,amsmath,amsthm,amssymb,graphicx}

\setlength{\baselineskip}{0.3in} \setlength{\parskip}{.05in}


\newcommand{\cvgindist}{\overset{\text{d}}{\longrightarrow}}
\DeclareMathOperator{\PR}{Pr} 
\DeclareMathOperator{\cov}{Cov}


\newcommand{\sX}{{\mathsf X}}
\newcommand{\tQ}{\tilde Q}
\newcommand{\cU}{{\cal U}}
\newcommand{\cX}{{\cal X}}
\newcommand{\tbeta}{\tilde{\beta}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\txi}{\tilde{\xi}}




\title{Design Document for R Package glmm}

\author{Christina Knudson}

\begin{document}
\maketitle{}

\begin{abstract}
This design document describes the process of performing Monte Carlo maximum likelihood (MCML) for generalized linear mixed models.  First, penalized quasi-likelihood (PQL) estimates are calculated, which help generate simulated random effects. Then, the Monte Carlo likelihood approximation (MCLA)  is calculated using the simulated random effects. Next, the MCLA is maximized to find Monte Carlo maximum likelihood estimates, the corresponding Fisher Information, and other statistics. Additional inference is then possible, including confidence intervals for the parameters.
\end{abstract}

\section{Theory}

Let $y=(y_1, \ldots, y_n)'$ be a vector of observed data. Let $u=(u_1,\ldots,u_q)'$ be a vector of unobserved, normally-distributed random effects centered at 0 with variance matrix $D$. Let $\beta$ be a vector of $p$ fixed effect parameters and let $\nu$ be a vector of $T$ variance components for the random effects so that $D$ depends on $\nu$. Let $\theta=(\beta ,  \nu)'$ be a vector containing all unknown parameters. Then the data $y$ are distributed conditionally on the random effects according to $f_\theta(y|u)$ and the random effects are distributed according to $f_\theta(u)$. Although $f_\theta(u)$ does not actually depend on $\beta$ and $f_\theta(y|u)$ does not depend on $\nu$, we write both densities with $\theta$ to keep notation simple in future equations.

Since $u$ is unobservable, the log likelihood must be expressed by integrating out the random effects:
\begin{align}
l(\theta)=\log \int f_\theta(y|u) f_\theta(u) \; du
\end{align}
For most datasets, this integral is intractible. In these cases, performing even basic inference on the likelihood is not possible. Rather than evaluating the integral, \citet{geyer:thom:1992} suggest using a Monte Carlo approximation to the likelihood. Monte Carlo likelihood approximation (MCLA) uses an importance sampling distribution $\tilde{f}(u)$ to generate random effects $u_k, k=1, \ldots, m$ where $m$ is the Monte Carlo sample size.  MCLA theoretically works for any $\tilde{f}(u)$, but the  $\tilde{f}(u)$ chosen for this package is the mixture distribution specified in section \ref{sec:genRand}.

Then the Monte Carlo log likelihood approximation is
\begin{align}
l_{m}(\theta) &=\log \dfrac{1}{m} \sum_{k=1}^mf_\theta(y|u_k)  \dfrac{ f_\theta(u_k)   }{\tilde{f}(u_k)}\\
&= \log \dfrac{1}{m} \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}. \label{eq:MCLAval}
\end{align}
When $\tilde{f}$ does not depend on $\theta$, the gradient vector of the MCLA with respect to $\theta$ is
\begin{align}
\nabla l_m(\theta)&= \dfrac{\sum_{k=1}^m     \nabla \log f_\theta(u_k,y)  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }{\sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}  }, \label{eq:MCLAgradient}
\end{align}
and the Hessian matrix of the MCLA is
\begin{align}
\nabla^2 l_m(\theta)&= \dfrac{   \sum_{k=1}^m  \nabla^2 \log f_\theta(y,u_k)       \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \nonumber\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)   - \nabla l_m(\theta)   \right] \left[ \nabla \log f_\theta(y,u_k)  - \nabla l_m(\theta)  \right]'  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}   }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \label{eq:MCLAhessian}
\end{align}
Details for these derivatives can be found in section \ref{sec:calcsindep}. Calculation for the gradient and Hessian when $\tilde{f}$ is dependent on $\theta$ can be found in section \ref{sec:calcs}.



Now any likelihood-based inference, such as maximum likelihood, can be performed on $l_m(\theta)$ and its derivatives.  









\section{Model fitting function} 
The model fitting function is be the primary function the user  uses. The user  specifies the response and the predictors using the R formula mini-language as interpreted by \texttt{model.matrix}. Let $n$ be the observed sample size and recall that $p$ is the number of fixed effects.  As a result of the user specifying the fixed effects, the model fitting function creates matrix $X$, which has dimensions $n \times p$. 

The user  specifies the random effects in the same way as the fixed effects. This is also how users of  \texttt{reaster} (of R package
 \texttt{aster} \citep{aster-package}) specify the random effects. That is, random effects are expressed using the R formula mini-language. Let $q_t$ be the number of random effects associated with variance component $\nu_t$. When the random effects are specified, a list of $T$ model matrices are created. The $t$th model matrix $Z_t$ has dimensions $n \times q_t$.


The user also specifies the expontial family (details in section \ref{sec:fam}), the name of the data set, and the names of the variance components.  

Thus, the following is a sample command with fixed predictors $x_1$ and $x_2$ and with random effects created by categorical variables $school$ and $classroom$:
\begin{verbatim}
glmm(y ~ x1+ x2, list(~0+school,~0+classroom),  family.glmm="bernoulli.glmm", 
data=schooldat,varcomps.names=c("school","classroom"),varcomps.equal=c(1,2),
debug=FALSE)
 \end{verbatim} 

It is possible that the user could want some variance components to be set equal. For example, \pcite{coull:agresti:2000} influenza dataset contains four years with random effects for each year. The authors want the within-year variance components to be equal.  There are also variance components for subject-specific intercepts and for the decreased susceptibility to illness in year 4 (since the strain of flu during year 4 was a repeat of a previous year). Suppose \texttt{year} is a categorical variable with four levels, and \texttt{year1} through \texttt{year4} are dummy variables. Thus, the call  contains these arguments:
\begin{verbatim}
glmm(y~year,list(~0+subject,~0+year1,~0+year2,~0+repeat,~0+year3,~0+year4),
 varcomps.equal=c(1,2,2,3,2,2), varcomps.names=c("subject","year","repeat"),
data=flu,family.glmm=bernoulli.glmm)
\end{verbatim}

Initially, \texttt{model.matrix}  makes $Z$ into a list of 6 design matrices. Since we have 3 distinct variance components, we want 3 design matrices. We take the design matrices that share a variance component and use \texttt{cbind} to bind them together. The result is 3 model matrices in the $Z$ list, one for each variance component. We  want to put them in order (1,2,3 according to \texttt{varcomps.equal}) so that the names for each model matrix  (``subject'',``year'',``repeat'') are correctly assigned.  The variance estimates  are eventually listed in the model summary in this same order as well.
    
  After interpreting the model the user has specified, the next step is to find penalized quasi-likelihood (PQL) estimates.  The process of finding these estimates is detailed in section \ref{sec:pql}. The PQL estimates parameterize the importance sampling distribution $\tilde{f}(u_k)$ which generate the random effects.  More information on generating the random effects is in section \ref{sec:genRand}. 

Next, \texttt{trust} is implemented to maximize  the MCLA objective function (details on the objective function are in section \ref{sec:objfun}). Finally, the function  returns parameter estimates, the log likelihood evaluated at those estimates, the gradient vector of the log likelihood,  the Hessian of the log likelihood at those estimates, information from the \texttt{trust} optimization, and other information.


%%Families%%
\section{Families} \label{sec:fam}


Let $g$ be the canonical link function and $\mu$ be a vector of length $n$ such that
\begin{align}
g(\mu) = X \beta + Z u
\end{align}
The choice of the link function is related to the distribution of the data, $ f_\theta(y|u)$. If the data have a  Bernoulli distribution, the link is $\logit(\mu)$. If the data have a Poisson distribution, the link is $\log (\mu)$. Currently, \texttt{glmm} offers only these two choices for the family, but any exponential family could work and can be easily added later.

 For simplicity of future notation, let $\eta=g(\mu)=X \beta + Z u$. Let $c(\eta)$ denote the cumulant function such that the log of the data density can be written as
\begin{align}
y' \eta - c(\eta) = \sum_i \left[ y_i \eta_i - c(\eta_i)  \right]
\end{align}

The user is required to specify the family in the model-fitting function. Once the family is specified, many family-specific functions are called. They are contained in an S3 class called ``glmm.family''. Each family function outputs a list including the family name (a character string such as ``bernoulli.glmm''), a function that calculates the value of the cumulant function $c(\eta)$,  a function that calculates the cumulant's first derivative $c'(\eta)$ with the derivative taken with respect to $\eta$, and  a function that calculates the cumulant's second derivative $c''(\eta)$. 

The users provide the family in the model-fitting function by either enter the character string (``bernoulli.glmm''), the function (\texttt{bernoulli.glmm()}), or the result of invoking the function.  The following code for using the input to determine the family is adapted from \texttt{glm}.

\begin{verbatim}
logDensity<-function(family.glmm)
{
	if(is.character(family.glmm))
		family.glmm<-get(family.glmm,mode="function",envir=parent.frame())
	if(is.function(family.glmm))
		family.glmm<-family.glmm()
	if(!inherits(family.glmm,"glmm.family")) 
		stop(" 'family.glmm' not recognized") 
	return(family.glmm)
}
\end{verbatim}
We interpret this as follows.  If the user has entered the family as a string, go get the R object with that family name, either from the immediate environment or the parent environment.  If this has happened, \texttt{family.glmm} is now a function.  If \texttt{family.glmm} is a function (either because the user entered it as a function or because of the preceding step), invoke that function.  At this point, \texttt{family.glmm} should have  class ``\texttt{glmm.family}.'' If this is not the case (maybe because of a typo or maybe because they entered ``\texttt{poisson}'' rather than ``\texttt{poisson.glmm}''), then stop and return an error.

With this family of functions, calculating $c(\eta_i), c'(\eta_i),$ and  $c''(\eta_i)$ is as simple as: 
\begin{verbatim}
family.glmm$cum(args)
family.glmm$cp(args)
family.glmm$cpp(args)
\end{verbatim}

For the Bernoulli distribution, we calculate these values (\texttt{cum}, \texttt{cp}, and \texttt{cpp}) as follows. In order to be careful with our computer arithmetic, we use the \texttt{log1p} function and the second set of equalities.  
\begin{align}
c(\eta_i) &= \log(1+e^{\eta_i}) =
  \begin{cases}
    \log(1+e^{\eta_i}) & \text{if } \eta_i\leq 0,\\
    \eta_i+\log(e^{-\eta_i}+1) & \text{if } \eta_i >0,
  \end{cases}\\
c'(\eta_i)&=\dfrac{ e^{{\eta_{i}}}}{ 1+e^{{\eta_{i}}}} = \dfrac{1}{1+e^{-\eta_i}}\\
c''(\eta_i)&=   \dfrac{e^{{\eta_{i}}}}{  1+ e^{{\eta_{i}}} }  - \dfrac{e^{2{\eta_{i}}}}{    (  1+ e^{{\eta_{i}}})^2}  = \dfrac{1}{1+e^{-\eta_i}}\cdot \dfrac{1}{1+e^{\eta_i}}
\end{align}


 For Poisson, these values (\texttt{cum}, \texttt{cp}, and \texttt{cpp}) are
\begin{align}
c(\eta_i)&=e^{\eta_i}\\
c'(\eta_i)&=e^{{\eta_{i}}}\\
c''(\eta_i)&= e^{{\eta_{i}}}.
\end{align}



Then we use these pieces to create the scalar $c(\eta)$, the vector $c'(\eta)$ and the matrix $c''(\eta)$. We calculate

\begin{align}
c(\eta)= \sum_i c(\eta_i).
\end{align}
 The vector $c'(\eta)$ has components $c'(\eta_i)$. The matrix $c''(\eta)$ is diagonal with diagonal elements $c''(\eta_i)$.

In the R function \texttt{glm}, the user can choose the link. The canonical link must be used in the \texttt{glmm} package so that we have an exponential family. The canonical link is included in \texttt{family.glmm}  in case the user does not know it already.

Also, this family of functions contains a check to ensure the data are valid given the family type. If \texttt{family.glmm} is \texttt{bernoulli.glmm}, the data should contain only $0$ and $1$. If \texttt{family.glmm} is \texttt{poisson.glmm}, then the data should be nonnegative.  If the data set does not pass the check, the check returns an error message. 

\subsection{Redone in C}
Redone in C, I  have  separate functions for calculating $c(\eta)$, $c'(\eta)$, $c''(\eta)$: \texttt{cum3.c}, \texttt{cp3.c}, and \texttt{cpp3.c}. The inputs for each function are identical:  \texttt{eta} (an array of doubles), $\texttt{neta}$ (the length of \texttt{eta}), the type (to denote the family: $1$ indicates Bernoulli and $2$ indicates Poisson), and an array of doubles to contain the result. These are passed in as pointers. The functions  calculate  the cumulant function or one of its first two derivatives. Each function  contains a switch statement for the glmm family. The calculations for each of these functions have  been shown earlier in this section. This function is type void: rather than returning the cumulant or its derivatives, the pointers are changed to contain the results. This function is invoked by \texttt{el}, described in section \ref{sec:el}.


\section{Log density of the data (\texttt{el})}\label{sec:el}
This section provides details for the log density of the data and two of its derivatives.  

\subsection{Equations}
Recall the log of the data density is
\begin{align}
\log f_\theta(y|u) &= y' \eta -c(\eta) \\
&= \sum_{i} y_{i} {\eta_{i}} - c({\eta_{i}})
\end{align}
where
\begin{align}
\eta=X\beta+Zu.
\end{align}
The derivative of this with respect to one component, $\eta_j$, is
\begin{align}
\dfrac{\partial}{\partial \eta_j} \log f_\theta(y|u)  = y_j-c'(\eta_j).
\end{align}
The derivative of the component $\eta_j$ with respect to one of the fixed effect predictors, $\beta_{l}$, is
\begin{align}
\dfrac{\partial \eta_j}{\partial \beta_{l}} = X_{j{l}}
\end{align}

We'd like the derivative of the log of the data density with respect to $\beta$. The first step is using the chain rule as follows:
\begin{align}
\dfrac{\partial}{\partial \beta_{l}}  \log f_\theta(y|u) &= \dfrac{\partial \eta_j}{\partial \beta_{l}} \dfrac{\partial}{\partial \eta_j} \log f_\theta(y|u) \\
&= \left[ y_j-c'(\eta_j) \right]  X_{j{l}}
\end{align}

The mixed partial derivative (with respect to $\beta_{l_1}$ and $\beta_{l_2}$) of the log data density can be written similarly:
\begin{align}
\dfrac{\partial^2}{\partial \beta_{l_1} \partial \beta_{l_2}}  \log f_\theta(y|u) &=\dfrac{\partial}{\partial \beta_{l_2}} \left( \left[ y_j-c'(\eta_j) \right]  X_{j{l_1}} \right) \\
&= \dfrac{\partial \eta_j}{\partial \beta_{l_2}} \dfrac{\partial}{\partial \eta_j} \left( \left[ y_j-c'(\eta_j) \right]  X_{j{l_1}} \right) \\
&= -X_{j{l_1}} X_{j{l_2}} c''(\eta_j) 
 \end{align}


Letting $c'(\eta)$ be a vector with components $c'(\eta_j)$, the first derivative of the log data density can be written in matrix form as:
\begin{align}
\dfrac{\partial}{\partial \beta}  \log f_\theta(y|u) = X' \left[ y-  c'(\eta)  \right].
\end{align}

Letting $ c''(\eta)$ be a diagonal matrix with components $c''(\eta_j)$, the second derivative of the log data density can be written in matrix form as:
\begin{align}
   \frac{\partial^2}{\partial \beta^2} \log f_\theta(y|u) =   X' [ -  c''(\eta) ] X
\end{align}

\subsection{Redone in C}
The C function to calculate the value of the log data density and its two derivatives are called by reference. The following pointers are passed in: double \texttt{Y}, double \texttt{X}, int \texttt{nrowX}, int \texttt{ncolX}, double \texttt{eta}, int \texttt{family}, double \texttt{elval}, double \texttt{elgradient}, double \texttt{elhessian}. The pointers  \texttt{elval}, \texttt{elgradient} and \texttt{elhessian} are zeros before \texttt{el.C} is invoked. Invoking \texttt{el.C}  then places the calculatedvalue of the log data density and two derivatives into \texttt{elval}, \texttt{elgradient} and \texttt{elhessian}.\\

The function \texttt{el.C} calls the following C functions: \texttt{cum3.C} to calculate the cumulant given a value of $\eta$, \texttt{cp3.C} to calculate the derivative of the cumulant given a value of $\eta$, \texttt{cpp3.C} to calculate the Hessian of the cumulant given a value of $\eta$, and functions to perform matrix multiplication.


\section{Random effect generation and calculations}
This section is focused on a vector of random effects $u$ with length $q$.  Section \ref{sec:getEk} details the  relationship between  $\nu$ and $D$.
Section \ref{sec:ftwiddle} states the importance sampling distribution.
 Section \ref{sec:genRand} describes the process of generating the random effects. Section \ref{sec:distRand} explains how to evaluate the distribution of the random effects and its first two derivatives.

%%getEk
\subsection{Constructing $D$ (\texttt{getEk})}\label{sec:getEk}
Recall that, for this version of the package, we assume that $D$ is diagonal.  Let $\nu$ be length $T$ with components $\nu_t$, and let $E_t,t=1,\ldots,T$ be diagonal matrices with indicators on the diagonal so that $\sum_{t=1}^T E_t = I$. That is, the diagonal entries  of $E_t$ indicate whether that random effect has $\nu_t$ as its variance component. Then $D=\sum_{t=1}^T\nu_t  E_t $.  

Recall $q_t$ is the number of random effects associated with variance component $\nu_t$. Then $q_t$ is also the number of nonzero entries in $E_t$ and $q=\sum_{t=1}^T q_t$ is the total number of random effects in the model.  
The model fitting function of \texttt{glmm} has made  \texttt{z} into a list with $T$ design matrices (one for each distinct variance component).  In order to create $E_t$, we need to  count the  number of columns ($q_t$) for each design matrix in the list.
Since $D$  is $q \times q$, each $E_t$  is $q \times q$.   $E_1$ has $q_1$ ones on the diagonal, followed by zeros to fill the rest of the diagonal.  $E_2$ has $q_1$ zeros, then $q_2$ ones, then zeros for the rest of the diagonal. We continue this process to complete the construction of $E_t, t=1,\ldots, T$.

\subsection{Importance sampling distribution $\tilde{f}(u_k)$}\label{sec:ftwiddle}
 Let $s$ be a vector of length $q$ that represents the random effects on the standard normal scale. That is $s$ is defined such that, $u= D^{1/2}s$. Let $\sigma$ be a vector of length $T$ with components $\sqrt{\nu_t},t=1,\ldots,T$.  Prior to generating random effects, the model fitting function of \texttt{glmm} has performed PQL and recorded the PQL estimates for $\beta$, $\sigma$ and  $s$. The PQL estimates are denoted by $\beta^*$, $s^*$ and $\sigma^*$. Let
\begin{align}
A^*=\sum_{t=1}^T E_t \sigma^*_t
\end{align}\
and
\begin{align}
 D^* =A^*A^*
\end{align}
be matrices based on PQL estimates. We can ``unstandardize'' our PQL-based random effects:
\begin{align}
u^*&=A^*s^*.
\end{align}


Let $p_1,p_2,p_3$ be the proportions of the mixture distribution such that  $p_1+p_2+p_3=1$.  Let $f(u|\mu,\Sigma)$ denote the pdf for $u \sim N(\mu,\Sigma)$.  Let $\grave{f}(u|0,D^*)$ denote the pdf for $u \sim t_\zeta$, a $q$-dimensional multivariate  $t$ with mean $0$, scale matrix $D^*$, and $\zeta$ degrees of freedom. Then the importance sampling distribution is:
\begin{align}
 \tilde{f}(u) = p_1  \grave{f}(u|0,D^*)+p_2  f(u \, | \, u^*, D^*)+p_3  f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1}). \label{eq:ftwiddle}
\end{align}


The first component of the mixture distribution $t_\zeta$ is chosen to ensure  the gradient of the MCLA has a central limit theorem, which is proven in  section \ref{sec:CLT}.  The second component is chosen because it is centered at the PQL best guess of the random effect values $u^*$ and  has the PQL guess of the variance. The last component is centered at the PQL guess $u^*$ and has a variance based on the Hessian of the PQL penalized likelihood.  The idea of this last distribution is to generate random effects from a distribution whose Hessian matches that of the target distribution $f_\theta(y,u)$.

\subsection{Generating random effects (genRand)}\label{sec:genRand}

 Recall that $m$ is the overall Monte Carlo sample size.  Let $p_1,p_2,p_3$ be the proportions of the mixture distribution such that  $p_1+p_2+p_3=1$. We randomly determine $m_1,m_2,m_3$ according to the proportions $p_1,p_2,p_3$. More specifically, we generate $m$ random numbers from a distribution that is uniform between $0$ and $1$. Then $m_1$ is the frequency with which the randomly generated numbers are between $0$ and $p_1$,  $m_2$ is the frequency with which the randomly generated numbers are between $p_1$ and $p_1+p_2$, and $m_3=m-p_1-p_2$.
%Let $D$ and $A=D^{1/2}$ be determined by the current iteration of \texttt{trust}.

Once $m_1, m_2$ and $m_3$ are determined, we draw 
\begin{align}
u_k &\sim t_\zeta ,k=1,\ldots,m_1\\
u_k &\sim N(u \, | \, u^*, D^*) ,k=m_1+1,\ldots,m_1+m_2\\
u_k &\sim N(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1}) ,k=m_1+m_2+1,\ldots,m.
\end{align}
Details for drawing from a nonstandard normal are in section \ref{sec:gennonstand}. We use R package \texttt{mvtnorm} to draw from a $t$ distribution.
 





\subsubsection{Finding the square root of a variance matrix}\label{sec:sqrtmat}
In order to generate draws from a nonstandard normal distribution, we need to calculate the square root of a variance matrix. When the variance matrix is diagonal (for example, $D^*$), we simply take the root of the diagonal elements.  When the variance matrix is not diagonal (for example, in the second component of the mixture distribution), we can use eigendecomposition. Eigendecompositions take a little bit more time than Cholesky decompositions but are more stable. 

Let $\Sigma$ denote the nondiagonal variance matrix. Eigendecomposition of $\Sigma$ provides orthogonal matrix $O$ (containing the eigenvectors) and diagonal matrix $\Lambda$ (with diagonal entries $\lambda$ being the eigenvalues of $\Sigma$).  Then
\begin{align}
\Sigma^{1/2}= O \Lambda^{1/2} O'.
\end{align}
Finding $\Lambda^{1/2}$ is as easy as taking the root of the diagonal entries.  We know that $\Sigma^{1/2}$ is correct because
\begin{align}
\Sigma^{1/2} \Sigma^{1/2} &= O \Lambda^{1/2} O'O \Lambda^{1/2} O'\\ \nonumber
&= O \Lambda^{1/2}\Lambda^{1/2} O'\\\nonumber
&=O \Lambda O' \\\nonumber
&= \Sigma.
\end{align}

\subsubsection{Generating from nonstandard normal distributions}\label{sec:gennonstand}
 Construct $A^*=(D^*)^{1/2}$ and $((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})^{1/2} $ as described in section \ref{sec:sqrtmat}. 

Let $\breve{u}_k,k=m_1+1\ldots,m$ be vectors of random effects that are drawn from the standard normal distribution. Then we center and scale $\breve{u}_k$ as follows:
\begin{align}
u_k&= u^*+A^* \, \breve{u}_k, k=m_1+1,\ldots,m_1+m_2\\
u_k&= u^*+ ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})^{1/2} \,  \breve{u}_k, k=m_1+m_2+1,\ldots,m.
\end{align}
 The result or centering and scaling is that
\begin{align}
 &u_k \sim N(u^*,D^*), \, k=m_1+1,\ldots,m_1+m_2\\
& u_k \sim N(u^*,(Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1}), \, k=m_1+m2+1,\ldots,m .
\end{align}

% 
%\subsubsection{NEED TO PUT THIS SOMEWHERE}
%
%
%Recall that $D$ is a diagonal with dimension $q$. $D$ has $q_1$ entries being $\nu_1$, $q_2$ entries being $\nu_2$, ..., and $q_T$ entries being $\nu_T$. Since the distribution of $u_k=A \breve{u}_k$ is $N(0,D)$, we write its pdf as:
%\begin{align}
%f(u_k|\nu)=f(A\breve{u}_k|\nu)&= (2 \pi)^{q/2} |D|^{-1/2} \exp(-0.5 (A \breve{u}_k)' D^{-1} (A \breve{u}_k))\\
%&=(2 \pi)^{q/2} |A|^{-1} \exp(-0.5 \breve{u}_k' A D^{-1} A \breve{u}_k)\\
%&=(2 \pi)^{q/2} |A|^{-1} \exp(-0.5 \breve{u}_k' \breve{u}_k)
%\end{align}
%So that 
%\begin{align}
%\log f(A \breve{u}_k|\nu) &\propto - \log |A| -\dfrac{1}{2} \breve{u}_k' \breve{u}_k\\
%&\propto -\dfrac{q_1}{2} \log \nu_1 +\ldots + -\dfrac{q_T}{2} \log \nu_T -\dfrac{1}{2} \breve{u}_k' \breve{u}_k
%\end{align}
%Then 
%\begin{align}
%\dfrac{\partial}{\partial \nu_t} \log f(A \breve{u}_k|\nu) = \dfrac{-q_t}{2 \nu_t}\\
%\dfrac{\partial^2}{\partial \nu_t^2} \log f(A \breve{u}_k|\nu) = \dfrac{q_t}{2 \nu_t^2}
%\end{align}
%
%Because only the first component of the mixture distribution contains any parameters,
%\begin{align}
%\dfrac{\partial}{\partial \nu} \log \tilde{f}(u) = \dfrac{p_1  \dfrac{\partial}{\partial \nu} N(u \, | \, 0, D)}{ p_1  N(u \, | \, 0, D)+p_2  N(u \, | \, u^*, D^*)+p_3  N(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})}.
%\end{align}
%and 
%\begin{align}
%\nabla  \log \tilde{f}(u) = \left(0   \; \; \; \; \; \; \dfrac{\partial}{\partial \nu} \log \tilde{f}(u) \right)
%\end{align}
%These are used in equations \ref{eq:MCLAgr} and \ref{eq:MCLAh}. The second derivatives, which are needed in equation \ref{eq:MCLAh}, are
%\begin{align}
%\dfrac{\partial^2}{\partial \nu^2} \log \tilde{f}(u) &= \dfrac{p_1  \dfrac{\partial^2}{\partial \nu^2} N(u \, | \, 0, D)}{ p_1  N(u \, | \, 0, D)+p_2  N(u \, | \, u^*, D^*)+p_3  N(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} \\ \nonumber
%&- \dfrac{p_1^2 \left( \dfrac{\partial}{\partial \nu} N(u \, | \, 0, D)\right) \left( \dfrac{\partial}{\partial \nu} N(u \, | \, 0, D)\right)'}{\left[ p_1  N(u \, | \, 0, D)+p_2  N(u \, | \, u^*, D^*)+p_3  N(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})\right]^2}.
%\end{align}
%and
%\begin{align}
%\nabla^2 \log \tilde{f}(u_k) = \begin{bmatrix} 0 & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log \tilde{f} (u_k)  \end{bmatrix} 
%\end{align}
%
%

%%
%%This section is useful for calculating $\tilde{f}(u_k)$.  This section also provides two derivatives in some cases, which is useful for evaluating $f_\theta(u_k)$ and two derivatives. These are needed for the calculation in \eqref{eq:MCLA}.
%%
%%Section \ref{sec:tee} contains details that are not currently implemented, but could be. In this version, all random effects are assumed to be normally distributed and all simulated random effects are generated from a normal distribution.


\subsection{Evaluating the distribution of random effects (\texttt{distRand})}\label{sec:distRand}
This section discusses evaluating the distribution of the random effects, the gradient of the random effects, and the Hessian of the random effects. That is, the equations in this section provide  $\log f_\theta(u)$, $\nabla \log f_\theta(u)$, and $\nabla^2 \log f_\theta(u)$  for equation \ref{eq:MCLA}.

 We can  express the log unnormalized pdf of $N(0, D)$  as:
\begin{align}
\log f_\theta(u)= \log f (u| 0, D) = - q/2 \log (2 \pi) -1/2 \log |D| - (1/2) u' D^{-1} u
\end{align}

 To find derivatives of this, we are going to use the diagonal form of $D$. Recall that for every $\nu_t$, we can construct a matrix $E_t$ (with dimensions the same as matrix $D$) that has 1s on the diagonal elements corresponding to the elements of D that contain $\nu_t$ and 0s  elsewhere.  

We can partition the random effects according to their variance components: $u=(U_1',...,U_T')'$.  Let $D_t$ be the variance matrix for $U_t$. $D_t$ has $q_t$ rows  and $q_t$ columns. Thus $D$ can be expressed as:
\begin{align}
D = \begin{bmatrix} D_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & D_T \end{bmatrix}
\end{align}

Since $D$ is diagonal, it follows that $D^{-1}$ is also diagonal with diagonal entries $\dfrac{1}{\nu_1}$,...,$\dfrac{1}{\nu_T}$.  Also, the assumption that $D$ is diagonal makes calculating the determinant of $D$  easy:
\begin{align}
|D|= \nu_1^{q_1}...\nu_T^{q_T}
\end{align}

 Taking these two pieces of information into account allows us to write the log density for $U$ as follows:
\begin{align}
\log f_\theta(u) &= -\dfrac{1}{2} \log |D| -\dfrac{1}{2} u' D^{-1} u\\
&= -\dfrac{1}{2} \left[  \sum_{t=1}^T q_t \log \nu_t   \right]  -\dfrac{1}{2} \sum_{t=1}^T \left[ \dfrac{1}{\nu_t} U_t'U_t   \right]
\end{align}



The first and second derivatives of each summand with respect to its associated $\nu_t$ are:
\begin{align}
\dfrac{\partial}{\partial \nu_t} \log f_\theta(u_t) = - \dfrac{q_t}{2 \nu_t} + \dfrac{1}{2 \nu_t^2}U_t'U_t
\end{align}
and 
\begin{align}
\dfrac{\partial^2}{\partial \nu_t^2} \log f_\theta(u_t) = \dfrac{q_t}{2 \nu_t^2}- \dfrac{1}{\nu_t^3} U_t'U_t.
\end{align}
Any other derivative is equal to 0. That is, for all $t_1 \neq t_2$,
\begin{align}
\dfrac{\partial}{\partial \nu_{t_1}} \log f_\theta(u_{t_2}) = 0.
\end{align}
Also,
\begin{align}
\dfrac{\partial}{\partial \beta} \log f_\theta(u_{t_2}) = 0.
\end{align}
Thus, if $\nu = (\nu_1,...,\nu_T)$, the gradient of the random effects distribution is the following vector of length $T$:
\begin{align}
\dfrac{\partial}{\partial \nu}  \log f_\theta(u) = \begin{bmatrix} - \dfrac{q_1}{2 \nu_1} + \dfrac{1}{2 \nu_1^2} U_1'U_1 & ... & - \dfrac{q_T}{2 \nu_T} + \dfrac{1}{2 \nu_T^2} U_T'U_T   \end{bmatrix} 
\end{align}

The Hessian matrix is the following diagonal matrix:
\begin{align}
\dfrac{\partial^2}{\partial \nu^2} \log f_\theta(u) = \begin{bmatrix} \dfrac{q_1}{2 \nu_1^2}- \dfrac{1}{\nu_1^3} U_1'U_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \dfrac{q_T}{2 \nu_T^2}- \dfrac{1}{\nu_T^3 U_T'U_T} \end{bmatrix}
\end{align}
%I will create another function to take $q_t$, $U_t$, $\mu_t$ and $\nu_t$ and spit out $\dfrac{q_t}{2 \nu_t^2} - \dfrac{1}{\nu_t^3}  (U_t-\mu_t)' (U_t-\mu_t)$. Again, during the above loop of length T, I will calculate the entries for the diagonal of this Hessian matrix.

To calculate the value, gradient, and Hessian, we need to provide  $\nu$, $u$, and the list \texttt{z} (from \texttt{mod.mcml}).  The list \texttt{z} has $T$ matrices, each with the number of columns equal to $q_t$. We need $q_t,t=1,\ldots,T$ to calculate the log density and its derivatives.



The last thing we need to discuss is how to split $U$ into $U_1,...,U_T$. We know that the first $q_1$ items of U are $U_1$, the next $q_2$ items are $U_2$, etc.  In other words, entries 1 through $q_1$ are $U_1$. Items $q_1+1$ through $q_1+q_2$ are $U_2$, etc. We find these numbers ($q_1$, $q_2$, etc) from the number of columns of the items in the list \texttt{z}. This information of how to split $u$ up is contained in vector \texttt{meow} and used by function \texttt{distRand3}. This is faster than recalculating \texttt{meow} for every call of \texttt{distRand3}. 

Rewritten in C, this function takes the following as pointers: the double array \texttt{nu} that contains the variance components, the int \texttt{T} to specify the length of \texttt{nu}, the double array \texttt{mu} that contains the means of the random effects, the int array \texttt{nrandom} of length \texttt{T} that contains the number random effects from that variance component, the double array \texttt{Uvec} that contains one vector of generated random effects,  the int array \texttt{meow} that specifies how to split \texttt{u} up based on the variance components, the double array \texttt{drgradient} that  contains the resulting gradient, and the double array \texttt{drhessian} that  contains the resulting Hessian.

The result of invoking the C function \texttt{distRand3} is  the calculation of the gradient and Hessian for the distribution of the random effects. The value is  evaluated by the C function \texttt{distRandGeneral}, described in section \ref{sec:Dgeneral}.



\subsection{Evaluating the importance sampling distribution}
This section discusses evaluating the importance sampling distribution shown in \ref{eq:ftwiddle}. The  $\tilde{f}(u)$ calculation is used in equation \ref{eq:MCLA}.  Section \ref{sec:Dgeneral} describes how to evaluate the pdf for the normal components of $\tilde{f}(u)$. Section \ref{sec:tstuff} describes how to evaluate the pdf for the t component of $\tilde{f}(u)$. 

To be computationally stable, we need to be careful  with the addition of the terms in \ref{eq:ftwiddle} to prevent overflow. We know that
\begin{align}
 \tilde{f}(u) &= p_1  \grave{f}(u|0,D^*)+p_2  f(u \, | \, u^*, D^*)+p_3  f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1}) \\
&= p_1 \exp( \log  \grave{f}(u|0,D^*) ))+p_2 \exp( \log(  f(u \, | \, u^*, D^*) ))+ \nonumber\\
&+ p_3 \exp( \log( f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1}) )).
\end{align}
Set
\begin{align}
\tilde{a}= \max \left\{ \log  \grave{f}(u|0,D^*) ), \log  f(u \, | \, u^*, D^*) ), \log f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})  \right\}
\end{align}
Then
\begin{align}
 \tilde{f}(u) 
&= p_1 \exp\left( \log  \grave{f}(u|0,D^*)  -\tilde{a}\right)+p_2 \exp \left( \log  f(u \, | \, u^*, D^* )-\tilde{a} \right)+ \nonumber\\
&+ p_3 \exp\left[ \log f \left(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1} \right)  -\tilde{a} \right].
\end{align}
is more stable computationally.

%%distRandGeneral
\subsubsection{Normal distribution for general variance matrix $\Sigma$ (\texttt{distRandGeneral})}\label{sec:Dgeneral}
Let $\Sigma$ be a variance matrix and let $\mu$ be a mean vector. Consider $N(\mu, \Sigma)$.   We can  write the log pdf of this distribution  as:
\begin{align}
\log f (u| \mu, \Sigma) = -\dfrac{q}{2} \log (2 \pi) + \dfrac{1}{2} \log |\Sigma^{-1}| - \dfrac{1}{2} (u-\mu)' \Sigma^{-1} (u-\mu)
\end{align}
 The only part of this to discuss is $|\Sigma^{-1}|$. We can use eigendecomposition to make $\Sigma^{-1}=O \Lambda O'$ where $O$ is orthogonal and $\Lambda$ is the diagonal matrix with eigenvalues. Since orthogonal matrices have determinant $\pm 1$, then $|O||O'|=1$. Thus  
\begin{align}
|\Sigma^{-1}|&=|O'| \; |\Lambda| \; |O| \\
&= |O'| \; |O| \; |\Lambda| \\
&=|\Lambda|,
\end{align}
which is just the product of the eigenvalues. The log of the determinant is calculated beforehand to save time, since this function is called $3m$ times throughout each trust optimization iteration, where $m$ is again the Monte Carlo sample size.

This function is also rewritten in C with the following  passed in as pointers: double \texttt{Sigma.inv} $\Sigma^{-1}$, double \texttt{logdet} $\log |\Sigma^{-1}|$, int \texttt{nrow}, double \texttt{uvec} a vector of random effects, double {mu} $\mu$, and double \texttt{distRandGenVal}.







\subsubsection{t distribution}\label{sec:tstuff}

Consider $t_\zeta$. We can write the log unnormalized pdf of this distribution as:
\begin{align}
\log \grave{f}(u|0,D^*)&=  \log \left( \Gamma \left( \dfrac{\zeta+q}{2}  \right)   \right)-\log \left( \Gamma \left( \dfrac{\zeta}{2}  \right)   \right)  - \dfrac{q}{2} \log(\zeta \pi)\\
&- \dfrac{1}{2} \log |D^*|     -\left[ \dfrac{\zeta+q}{2} \right] \log \left(1+\dfrac{u'D^{* -1}u}{\zeta} \right)\\
\end{align}

The first four terms are constant with respect to $u$ and can be calculated ahead of time. Rewritten in C, we need to pass in  \texttt{tconstants} (the sum of the first four terms) as a double, \texttt{u} as an array of doubles, \texttt{zeta} as an int, \texttt{myq} as an int, the diagonal of \texttt{Dstarinv} as an array of doubles,  and \texttt{logft} as a double. The final calculation will be written into \texttt{logft}.

I have another function (written in R) that calculates the constants for the t distribution. The arguments of this function are \texttt{zeta}, the diagonal of $D^*$ \texttt{Dstardiag}, and the length of $u$ \texttt{myq}.


%\subsection{$\tilde{f}(u_k)$}
%I need to evaluate $\log \tilde{f}(u_k)$ in the objective function of trust. I can make a function using the general form for a normal distribution's density as described in the previous subsection, then use that function for the three distributions that I sampled from. Since I generated the random effects from the three distributions in proportions $p_1,p_2,p_3$, I can write 
%\begin{align}
%\log \tilde{f}(u_k) = p_1 \log f(u_k \, | \, 0, D^*)+p_2 \log f(u_k \, | \, u^*, D^*)+p_3 \log f(u_k \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})
%\end{align}
%
%\subsection{Distribution of random effects is multivariate t}\label{sec:tee}
%If we want, we can draw random effects from a t distribution with location parameter $u^*$ from PQL and scale matrix $D^*$ (using the variance components estimated by PQL). These can be drawn using the package mvtnorm. This package can also give the density evaluated at certain points. I'm not sure if I'll use this or just stick to normals.

%%%%%%%%%%%%%%%%%
%\section{t distribution for generating random effects}
%Generating random effects from a normal distribution can give us some random effects from the tails of the distribution, which results in importance sampling weights that are problematically big.  To try to fix this, we can change our importance sampling distribution to a multivariate t-distribution with location parameter $\mu$, scale parameter $D$, and $\gamma$ degrees of freedom.  The pdf is proportional to 
%\begin{align}
%f(U |\theta)=  |D|^{-.5} \left[ 1+\dfrac{1}{\gamma} (U-\mu)' D^{-1}(U-\mu)   \right]^{(\gamma+p)/2}
%\end{align}
%
%Then the log density is
%\begin{align}
%\log f(U |\theta) &\propto -.5 \log |D| - \dfrac{\gamma+q}{2} \log \left[ 1+ \dfrac{1}{\gamma} (U-\mu)' D^{-1} (U-\mu)  \right] \\
%& \propto \sum_{t=1}^T \left[- .5  q_t \log \nu_t \right]- \dfrac{\gamma+q}{2} \log \left[ 1+ \dfrac{1}{\gamma} (U-\mu)' D^{-1} (U-\mu)  \right]  \\
%& \propto \sum_{t=1}^T \left[- .5  q_t \log \nu_t \right]- \dfrac{\gamma+q}{2} \log \left[ 1+ \dfrac{1}{\gamma} \sum_{t=1}^T \dfrac{1}{\nu_t} (U_t-\mu_t)' (U_t-\mu_t)  \right]  \\\end{align}
%Its first derivative with respect to $\nu_t$ is
%\begin{align}
%\dfrac{\partial}{\partial \nu_t} \log f(U |\theta) &= \dfrac{- q_t}{2 \nu_t} + \dfrac{\gamma+q}{2} \dfrac{\dfrac{1}{\gamma \nu_t^2}(U_t-\mu_t)' (U_t-\mu_t)}{1+ \dfrac{1}{\gamma} \sum_{t=1}^T \dfrac{1}{\nu_t} (U_t-\mu_t)' (U_t-\mu_t)}
%\end{align}
%Its second derivative with respect to $\nu_t$ is
%
%\begin{align}
%\dfrac{\partial^2}{\partial \nu_t^2} \log f(U |\theta) &= \dfrac{q_t}{2 \nu_t^2} - \dfrac{\gamma+q}{2} \left[ \dfrac{\dfrac{1}{\nu_t^2 \gamma}(U_t-\mu_t)'(U_t-\mu_t)}{1+ \sum_{t=1}^T \dfrac{1}{\nu_t \gamma }(U_t-\mu_t)'(U_t-\mu_t) }  \right]^2 - \dfrac{\gamma+q}{2}  \left[ \dfrac{\dfrac{2}{\nu_t^3 \gamma}(U_t-\mu_t)'(U_t-\mu_t)}{1+ \sum_{t=1}^T \dfrac{1}{\nu_t \gamma}(U_t-\mu_t)'(U_t-\mu_t) }  \right]
%\end{align}



\section{Objective function: approximated log likelihood}\label{sec:objfun}
The objective function is optimized by \texttt{trust} within the the model fitting function.  In order to evaluate and maximize the approximated log likelihood, \texttt{trust} needs an objective function that returns the value, the gradient vector, and the Hessian matrix of the approximated log likelihood.  Recall equations \ref{eq:MCLAval}, \ref{eq:MCLAgradient}, and \ref{eq:MCLAhessian}   for calculating these quantities.  
%This objective function needs $\log f_\theta (u_k)$, $c(\eta_i)$, $\tilde{f}(u_k)$, each of their gradient vectors, and each of their Hessian matrices to plug into these expressions.    

Denote
\begin{align}
b_k = \log f_\theta (u_k) + \log f_\theta (y|u_k) - \log \tilde{f} (u_k). \label{eq:MCLA}
\end{align}


For computational stability, we'll set $a=$max$(b_k)$. Then the value of the MCLA is expressed as:\\
\begin{align}
\l_m (\theta) = a+ \log \left[ \dfrac{1}{m}  \sum_{k=1}^m e^{b_k-a} \right]
\end{align}

Define the weights as:
\begin{align}
v_\theta(u_k,y) = \dfrac{e^{b_k-a}}{ \sum_{k=1}^m e^{b_k-a}} 
\end{align}

Rewriting equation \ref{eq:MCLAgradient} using the notation for the weights, the gradient vector is: 
\begin{align}
\nabla l_m(\theta) &= \sum_{k=1}^m  \nabla \log f_\theta (u_k,y) v_\theta(u_k,y)\\ \label{eq:gradient}
&= \sum_{k=1}^m \nabla \left[  \log f_\theta(y|u_k) + \log f_\theta (u_k)   \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \nabla \log f_\theta(y|u_k) + \nabla\log f_\theta (u_k)  \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \;  \dfrac{\partial}{\partial \nu} \log f_\theta(y|u_k) \right] v_\theta(u_k,y)+ \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(u_k) \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right] v_\theta(u_k,y)\\
&=  \sum_{k=1}^m \left(  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \; 0 \right] + \left[ 0 \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right]   \right) v_\theta(u_k,y)   \\
&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \;  \dfrac{\partial}{\partial \nu} \log f_\theta(u_k)  \right] v_\theta(u_k,y) \label{eq:MCLAgr}
\end{align}

Then the Hessian matrix from equation \ref{eq:MCLAhessian} is expressed as: 
\begin{align}
&\nabla^2 l_m(\theta)=  \sum_{k=1}^m  \nabla^2 \log f_\theta(y,u_k)   v_\theta(u_k,y) \label{eq:MCLAh} \\ 
&+   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
    \nabla l_m(\theta)   \right] \left[ \nabla \log f_\theta(y,u_k)  -\nabla l_m(\theta)  \right]'  v_\theta(u_k,y)   . \nonumber
\end{align}
Everything in this has already been defined except:
\begin{align}
\nabla^2 \log f_\theta (u_k,y) &=  \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k,y) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k,y)  \end{bmatrix} \\
&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (y|u_k)  \end{bmatrix} \\
&= \begin{bmatrix} 0 & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0 & 0  \end{bmatrix} \\
&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} 
\end{align}



%HERE IS WHEN FTWIDDLE DEPENDS ON NU
%Rewriting equation \ref{eq:MCLAgradient} using the notation for the weights, the gradient vector is: 
%\begin{align}
%\nabla l_m(\theta) &= \sum_{k=1}^m \left( \nabla \log f_\theta (u_k,y) - \nabla \log \tilde{f}(u_k)  \right) v_\theta(u_k,y)\\ \label{eq:gradient}
%&= \sum_{k=1}^m \nabla \left[  \log f_\theta(y|u_k) + \log f_\theta (u_k) - \nabla \log \tilde{f}(u_k)  \right] v_\theta(u_k,y)\\
%&= \sum_{k=1}^m  \left[ \nabla \log f_\theta(y|u_k) + \nabla\log f_\theta (u_k) - \nabla \log \tilde{f}(u_k) \right] v_\theta(u_k,y)\\
%&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \;  \dfrac{\partial}{\partial \nu} \log f_\theta(y|u_k) \right] v_\theta(u_k,y)+ \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(u_k) \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right] v_\theta(u_k,y)\\
%&+  \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log \tilde{f}(u_k) \; \; \; \;  \dfrac{\partial}{\partial \nu}  \log \tilde{f}(u_k) \right] v_\theta(u_k,y) \\
%&=  \sum_{k=1}^m \left(  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \; 0 \right] + \left[ 0 \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right]  +\left[ 0 \; \; \; \; \dfrac{\partial}{\partial \nu} \log \tilde{f}(u_k) \right] \right) v_\theta(u_k,y)   \\
%&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \; \left( \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right)+\left( \dfrac{\partial}{\partial \nu} \log \tilde{f}(u_k) \right) \right] v_\theta(u_k,y) \label{eq:MCLAgr}
%\end{align}
%
%Then the Hessian matrix from equation \ref{eq:MCLAhessian} is expressed as: 
%\begin{align}
%&\nabla^2 l_m(\theta)=  \sum_{k=1}^m \left[ \nabla^2 \log f_\theta(y,u_k)  -
%   \nabla^2 \log \tilde{f}(u_k)    \right] v_\theta(u_k,y) \label{eq:MCLAh} \\ 
%&+   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
%   \nabla \log \tilde{f}(u_k) - \nabla l_m(\theta)   \right] \left[ \nabla \log f_\theta(y,u_k)  -
%   \nabla \log \tilde{f}(u_k) -\nabla l_m(\theta)  \right]'  v_\theta(u_k,y)   . \nonumber
%\end{align}
%Everything in this has already been defined except:
%\begin{align}
%\nabla^2 \log f_\theta (u_k,y) &=  \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k,y) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k,y)  \end{bmatrix} \\
%&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
%\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (y|u_k)  \end{bmatrix} \\
%&= \begin{bmatrix} 0 & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
%\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0 & 0  \end{bmatrix} \\
%&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} 
%\end{align}
%
%

\section{Summary of model}
Typing \texttt{summary(mod)}  provide a summary of the model. This is broken into two pieces (as is usually done for summaries in R):  \texttt{summary.glmm} and \texttt{print.summary.glmm}. The first contains more calculated results than the second. When a user types \texttt{summary(mod)}, only the  basic information is automatically printed. More information can be found in the summary list.

The summary performs all calculations and its value is a list of the following:
\begin{itemize}
\item the call.
\item a matrix with the predictor name in the first column, the estimated coefficient in the second column, the standard error in the third column, the z value in the fourth column, and the two-sided p-value  in the fifth column. All inference is asymptotic, so we use the standard normal distribution to calculate the p-values.
\item a matrix with the variance estimate name  in the first column, the estimated variance component in the second column, the standard error in the third column, the z value in the fourth column, and the one-sided p-value  in the fifth column. All inference is asymptotic, so we use the standard normal distribution to calculate the p-values.
\item the evaluated Monte Carlo log likelihood along with its first and second derivative.
\item $m_1$, $m_2$, $m_3$.
\item output from texttt{trust}, such as whether it converged. (See \texttt{trust} for more details.)
\item the generated random effects.
\end{itemize}

A note on the standard errors: to calculate the standard errors, we take the MCLA Hessian matrix, invert it, take the diagonal elements, and apply the square root to them. It is possible that the Hessian will be noninvertible if it is close enough to singular that the computer thinks it is singular.  Then the standard errors will all be infinite  the user is warned.

Then \texttt{print.summary.glmm}  prints the following:
\begin{itemize}
\item the call.
\item a matrix with the predictor name in the first column, the estimated coefficient in the second column, the standard error in the third column, the z value in the fourth column, and the two-sided asymptotic p-value  in the fifth column. 
\item a matrix with the variance estimate name  in the first column, the estimated variance component in the second column, the standard error in the third column, the z value in the fourth column, and the one-sided p-value  in the fifth column. All inference is asymptotic, so we use the standard normal distribution to calculate the p-values.
\end{itemize}

%I think for the printCoefmat to work, the fixed effects matrix needs to look like
%\begin{verbatim}
%            Estimate Std. Error z value     Pr(>|z|)
%(Intercept)        2       0.50       4 6.334248e-05
%x1                 2       0.25       8 1.244192e-15
%\end{verbatim}
%(or at least it worked when I had these as the column names and didn't work before that).

Note that the summary and print.summary functions are S3 generic functions.  This means is the user  types \texttt{summary(mod)} and \texttt{summary} is a generic function. R checks the class of the model \texttt{mod} and  automatically uses the \texttt{summary} and \texttt{print.summary} functions for that class of objects. 




\section{PQL}\label{sec:pql}
 This section on PQL requires a change in notation so that we can avoid constrained optimization.  Recall that $D=\var(u)$ and is assumed to be diagonal. Let $A=D^{1/2}$ so that A has diagonal components that are positive or negative. Using A rather than D enables unconstrained optimization.  If $\sigma$ is a vector of the distinct standard deviations with components $\sigma_t$, we can write A as a function of $\sigma$ by
\begin{align*}
A= \sum_t E_t \sigma_t.
\end{align*}
Recall that $E_t$ has a diagonal of indicators to show which random effects have the same variance components, and $\sum_{t=1}^T E_t$ is the identity matrix. PQL  estimates the components contained on the diagonal of A. Taking the absolute value of those components  provides the standard deviations (where the standard deviations are the square root of the variance components).

We also define $s$ where $u=As$. The purpose of using $s$ rather than $u$ is to avoid $D^{-1/2}$ in the objective functon that we optimize.


There are two variations of PQL, both of which are described in the vignette
 \texttt{re.pdf} in the R package \texttt{aster} (Geyer, 2014). Both variations have an inner optimization and an outer optimization. The inner optimization is a well behaved quadratic while the outer optimization is more challenging. I use the variation of PQL that is not the original version, but is pretty close and is better behaved.  In this version, the inner optimization finds $\tilde{\beta}$ and $\tilde{s}$ given $X$, $Z$ and $A$. Then, given $\tilde{\beta}$ and $\tilde{s}$, the outer optimization finds $A$.




The {\bf inner} optimization is done with the \texttt{trust} function in R. We chose to use \texttt{trust} because it requires two derivatives, which  make the optimization more precise. We would like more accuracy in the inner maximization because any imprecision carries into the outer optimization.

The inner optimization maximizes the penalized log likelihood. After defining
\begin{align}
\eta=X\beta +ZAs
\end{align}
we calculate the  log likelihood as 
\begin{align}
l(\eta)= y' \eta - c(\eta) 
\end{align}
and the penalized likelihood as:
\begin{align}
 l(\eta)- \dfrac{1}{2} s's.
\end{align}
Since this function is maximized using  \texttt{trust}, we need to provide derivatives with respect to $s$ and $\beta$. We express these via the multivariate chain rule, taking advantage of $\eta_i$.

Create  vector $\mu$ with components $\mu_i=c'(\eta_i)$.  Since
\begin{align}
l(\eta) = \sum_i Y_i \eta_i - c(\eta_i)
\end{align}
then 
\begin{align}
\dfrac{\partial \l(\eta)}{\partial \eta_i} = Y_i-c'(\eta_i) = Y_i -\mu_i.
\end{align}
 Now we can write the following expression:

\begin{align}
%\dfrac{\partial}{\partial s_k} \left[ l(\eta)-\dfrac{1}{2} s's  \right]= \sum_i (Y_i-\mu_i) \dfrac{\partial \eta_i}{\partial s_k} \\
\dfrac{\partial}{\partial \beta_k} \left[ l(\eta)-\dfrac{1}{2} s's  \right] &= \dfrac{\partial l(\eta)}{\partial \beta_k}    \\
&= \dfrac{\partial l(\eta)}{\partial \eta_i} \dfrac{\partial \eta_i}{\partial \beta_k}    \\
&=\sum_i (y_i-\mu_i) \dfrac{\partial \eta_i}{\partial \beta_k} \\
&=\sum_i (y_i-\mu_i) X_{ik}
\end{align}


We find the function's derivative with respect to $s$ as follows:
\begin{align}
\dfrac{\partial }{\partial s} \left[ l(\eta) -\dfrac{1}{2} s's   \right] &= \dfrac{\partial l(\eta)}{\partial s} -\dfrac{1}{2} \dfrac{\partial s's}{\partial s}\\
&= \dfrac{\partial l(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial s} -s\\
&= (y-\mu)' \left[ \dfrac{\partial}{\partial s} ZAs \right] -s\\
&=(y-\mu)'ZA -s
\end{align}

This gives us the following derivatives of the penalized log likelihood:
\begin{align}
\dfrac{\partial}{\partial \beta} \left[ l(\eta)-(1/2)s's \right]&= X' (y-\mu)\\
\dfrac{\partial}{\partial s} \left[ l(\eta)-(1/2)s's \right]&= AZ' (y-\mu)  -s
\end{align}

Lastly, we need the Hessian of the penalized likelihood. This matrix can be broken down into four pieces: 
\begin{enumerate}
\item $ \dfrac{\partial^2}{\partial s^2}$
\item $ \dfrac{\partial^2}{\partial \beta^2}$
\item $\dfrac{\partial^2}{ \partial s \; \partial \beta}$
\item $\left(\dfrac{\partial^2}{ \partial s \; \partial \beta}\right) ' =\dfrac{\partial^2}{ \partial \beta \; \partial s}$
\end{enumerate}

We start at the top with  $ \dfrac{\partial^2}{\partial s^2}$, which is a $q \times q$ matrix.
\begin{align}
  \frac{\partial^2}{\partial s^2}  \left[ l(\eta) - (1/2) s's   \right] &=   \frac{\partial}{\partial s} \left[ (y-c'(\eta))'ZA -s   \right]\\
&=\left[- \frac{\partial}{\partial s} c'(\eta)\right]'ZA - I_q \\
&= \left[-\dfrac{\partial c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial s}   \right] 'ZA - I_q \\
&= \left[ -c''(\eta) ZA  \right]'ZA - I_q \\
&= -AZ' \left[ c''(\eta) \right] ZA - I_q
\end{align}
Note that $c''(\eta)$ is a $q\times q$ diagonal matrix with diagonal elements $c''(\eta_i)$.  $I_q$ is the identity matrix of dimension $q$.  This makes  $ \dfrac{\partial^2}{\partial s^2}$ a $q \times q$ matrix.

Next  is  $ \dfrac{\partial^2}{\partial \beta^2}$, which is a $p \times p$ matrix.
\begin{align}
  \dfrac{\partial^2}{\partial \beta^2}  \left[ l(\eta) - (1/2) s's   \right] &=   \dfrac{\partial}{\partial \beta} \left[ X' (y-c'(\eta))  \right]\\
&= \dfrac{\partial}{\partial \beta} \left[ X' y-X'(c'(\eta))  \right] \\
&= \dfrac{\partial}{\partial \beta} \left[ -X'(c'(\eta))  \right] \\
&=-X  \left[' \dfrac{\partial}{\partial \beta} c'(\eta)  \right] \\
&=-X'  \left[ \dfrac{\partial  c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial \beta}  \right] \\
&=-X'  \left[ c''(\eta)\right]  X   
\end{align}

Next is the $p \times q$ mixed partial $\dfrac{\partial^2}{  \partial \beta \; \partial s}$.
\begin{align}
\dfrac{\partial^2}{ \partial \beta \;  \partial s}  \left[ l(\eta) - (1/2) s's   \right] &= \dfrac{\partial}{\partial \beta} \left\{  \left[ y-c'(\eta)  \right]' ZA  \right\}\\
&= \dfrac{\partial}{\partial \beta} \left\{y'ZA-  \left[ c'(\eta)  \right]' ZA  \right\} \\
&=- \dfrac{\partial}{\partial \beta} \left\{  \left[ c'(\eta)  \right]' ZA  \right\} \\
&=- \left[  \dfrac{\partial  c'(\eta)}{\partial \beta}  \right]' ZA  \\
&=-   \left[  \dfrac{\partial  c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial \beta}  \right]' ZA   \\
&=-   \left[ c''(\eta) X  \right]' ZA   \\
&=-X'   \left[ c''(\eta)   \right] ZA  
\end{align}


And last we have the $q \times p$ mixed partial $\dfrac{\partial^2}{  \partial \beta \; \partial s}= -AZ' [c''(\eta)] X.$  These four pieces specify the hessian matrix for the penalized likelihood. Now \texttt{trust} can perform the inner optimization to find $\tilde{\beta}$ and $\tilde{s}$. 





The {\bf outer} optimization is done using \texttt{optim} with the default method of ``\texttt{Nelder-Mead.}'' This requires just the function value and no derivatives.  This optimization method was chosen because the optimization function already contains second derivatives of the cumulant function; requiring derivatives of the optimization function would in turn require higher-order derivatives of the cumulant.  

The default of \texttt{optim} is to minimize, but we'd like to  do maximization. Reversing the sign of the optimization function will turn the maximization into minimization.

If $\tilde{\beta}$ and $\tilde{s}$ are available from previous calls to the inner optimization function, then they are used here. Otherwise, the values are taken to be 0 and 1. The outer optimization's function is evaluated by first defining
\begin{align}
\tilde{\eta} &= X \tilde{\beta} +ZA \tilde{s}\\
l(\tilde{\eta}) &= y' \tilde{\eta} - c(\tilde{\eta})\\
A&= \sum_k E_k \sigma_k.
\end{align}
Let $W$ be a diagonal matrix with elements $c''(\eta_i)$ on the diagonal. Then the quantity we would like to maximize is the penalized quasi-likelihood:
\begin{align}
 l(\tilde{\eta}) - \dfrac{1}{2} \tilde{s}' \tilde{s} - \dfrac{1}{2} \log  \left| AZ' W ZA +I  \right| 
\end{align}
Again, \texttt{optim} minimizes, so we have to minimize the negative value of the penalized quasi-likelihood in order to maximize the value of it.

We do need to be careful about the determinant. Let $AZ' W ZA +I= LL'$ where $L$ is the lower triangular matrix resulting from a Cholesky decomposition. Then
\begin{align}
\dfrac{1}{2} \log | AZ' W ZA +I | &= \dfrac{1}{2} \log |LL'|\\
&=\dfrac{1}{2} \log (|L|)^2 \\
&= \log |L|
\end{align}
Since $L$ is triangular, the determinant is just the product of the diagonal elements. Let $l_i$ be the diagonal elements of $L$. Then
\begin{align}
\dfrac{1}{2} \log | AZ' W ZA +I | &= \log \prod l_i \\
&= \sum \log l_i
\end{align}

 If I become worried about all variance components being zero, I could implement an eigendecomposition using the R function \texttt{eigen}. This would be more numerically stable, but is slower. Let $O$ be the matrix containing the eigenvectors and let $\Lambda$ be a diagonal matrix with the eigenvalues $\lambda_i$ on the diagonal. Then
\begin{align}
 &AZ' W ZA = O \Lambda O' 
\end{align}
Then we can rewrite the argument of the determinant as follows:
\begin{align}
& AZ' W ZA +I  = O \Lambda O' + I = O \Lambda O + OO'=O (I+\Lambda) O' 
\end{align}
This leads to the careful calculation of our determinant as follows:
\begin{align}
&   \left| AZ' W ZA +I \right| = 1 *  \prod_{i=1}^n (1+\lambda_i) *1\\
&\Rightarrow \log   | AZ' W ZA +I | = \sum \log(1+\lambda_i) 
\end{align}
The last quantity can be accurately calculated using the \texttt{log1p} function in R.

The outer optimization  uses $\tilde{\beta}$ and $\tilde{s}$ provided by the inner optimization, but it does not return them. To keep track of the most recent  $\tilde{s}$, so store them in an environment that I call \texttt{cache}.  The purpose of  $\tilde{\beta}$ and $\tilde{s}$ is two-fold. First, if they are available from a previous iteration of the inner optimization, then they are used in the outer optimization of PQL.  Second, after PQL is finished, $\tilde{s}$ is used to help center the generated random effects.


The point of utilizing PQL is to construct a decent importance sampling distribution. Thus, the estimates do not have to be perfect.  It is possible that one of the $\sigma_k$ will be 0 according to PQL. If this happens, then I just use $\sigma_k = .01$ for the importance sampling distribution.








\section{Checks}
\subsection{Checking the MCLA finite differences}
To check the function that calculates the MCLA $l_m(\theta)$, I use finite differences on the \citet{booth:hobert:1999} example. To do this, I chose a value of $\theta=(\beta,\sigma)$ and a relatively small value of $\delta$, where $\delta$ is a vector of length $2$. We can check that the value and first derivative of the MCLA function are calculated correctly by making sure the following approximation holds
\begin{align}
\nabla l_m (\theta)  \cdot \delta \approx l_m(\theta+\delta)-l_m(\theta).
\end{align} 
Then, we can check the first and second derivatives of the MCLA are calculated correctly by checking for the following approximation:
\begin{align}
\nabla^2 l_m (\theta) \delta \approx \nabla l_m (\theta+\delta)-\nabla l_m (\theta)
\end{align}

\subsection{Checking  functions using the Booth and Hobert example}
I also test the objective function by taking the  \citet{booth:hobert:1999} example and rewriting the functions for this specific example. I then compare the values produced by the check functions and the original functions. Functions checked this way are:
\begin{enumerate}
\item log of the data density (\texttt{el})
\item log of the density for the random effects (both \texttt{distRand} and \texttt{distRandGeneral})
\item the Monte Carlo likelihood approximation (\texttt{objfun})
\end{enumerate}

\subsection{Checking a putative MLE using MCMC}

Suppose $\hat{\theta}$ is claimed to be the MLE. For example, $\hat{\theta}$ could be the MCMLE.  Ideally, we would like to check whether the true likelihood $l(\theta)$ achieves a local max at $\hat{\theta}$.  Due to the intractible integral in the likelihood expression, the best we can do is make sure that  $\nabla l_m(\theta)$ evaluated at $\hat{\theta}$ is very close to $0$. 

Suppose $u_k,k=1,\ldots,m$ are sampled from importance sampling distribution $\tilde{f} (u_k)$. Recall that the gradient of the MCLA is:
\begin{align}
\nabla l_{m}(\theta) = \dfrac{\sum_{k=1}^m\frac{\nabla f_\theta(y|u_k) f_\theta(u_k)   }{\tilde{f}(u_k)}}{\sum_{k=1}^m \frac{  f_\theta(u_k)   }{\tilde{f}(u_k)}}
\end{align}
We can choose $\tilde{f}(u_k)=f_{\hat{\theta}}(u_k,y)$. If the putative MLE is truly the MLE, then $\theta=\hat{theta}$, which in turn implies that $\tilde{f}(u_k)=f_{{\theta}}(u_k,y)$.   To be clear, in this check  we no longer use a mixture of normals for $\tilde{f}(u_k)$.  With the selected importance sampling distribution, each of the importance sampling weights is equal to $1$ and the sum of the weights is $m$.  Therefore, the gradient of the MCLA simplifies as follows:
\begin{align}
\nabla l_{m}(\theta) &=\dfrac{1}{m} \; {\sum_{k=1}^m\dfrac{\nabla f_\theta(y|u_k) f_\theta(u_k)   }{\tilde{f}(u_k,y)}}\\
&= \dfrac{1}{m} \; {\sum_{k=1}^m\dfrac{\nabla f_\theta(u_k,y)   }{\tilde{f}(u_k,y)}} \\
&= \dfrac{1}{m} \; \sum_{k=1}^m \nabla \log f_\theta(u_k,y)
\end{align}
This shows that the gradient of the MCLA is the average of the gradient of the complete data log likelihood, as long as $\hat{\theta}$ is truly an MLE. We can produce $u_k,k=1,\ldots,m$, using Markov chain Monte Carlo. Using $u$ as the variable, w run the Markov chain (perhaps \texttt{metrop} from the R package \texttt{mcmc}) on the complete data log likelihood.  We then use these samples to calculate the gradient of the complete data log likelihood, which in turn calculates the gradient of the MCLA. 

 If we split the MCMC runs into batches, we can calculate the batch means of the MCLA gradient, the grand mean of the MCLA gradient, and the corresponding Monte Carlo standard error.  If the putative MLE truly maximizes the likelihood, then the MCLA gradient's components should be close to $0$. We can check that they are close enough to $0$ by comparing them to the Monte Carlo standard error.

%%%%%%LIKELIHOOD RATIO TEST STUFF
%\section{Likelihood ratio test}
%This is another S3 generic function that the user can choose to implement if they'd  like  to do likelihood ratio tests for nested models. Eventually I'll want this to handle an arbitrary number of models, but for now we'll stick to comparing two models: $mod2$ nested in $mod1$.  I will assume these models have already been fit. 
%
%
% There are a few ways that the two models could differ. In other words, we could be testing:
% \begin{enumerate}
% \item whether one or more fixed effects are zero.
% \item whether one variance component is zero.
% \item whether multiple variance components are zero.
% \item whether one or more fixed effects is zero and one or more variance components are zero. 
%\end{enumerate}
%The hypothesis testing procedure and method for calculating p-values are different for each of these cases, so I'll go through each one in the following subsubsections.
%
%\subsection{Testing whether one or more fixed effects are zero}
%Consider the  case of testing whether one or more fixed effects are zero and the random effects are the same between the two models. I think we just do a likelihood ratio test as we're familiar with for linear models.   Let the number of fixed effect parameters in the larger model be $p_1$  and the number in the nested model be $p_2$. I will need the log likelihood values of the models outputted (from the main function at the same time as when I output the MCMLEs and the Fisher information). The log likelihood is simply the ``value'' from the objective function that trust uses. Let's call the log likelihood from the larger model  $l_1$ and the log likelihood from the nested model  $l_2$. 
%
%Then the likelihood ratio test statistic is
%\begin{align}
%t_{LRT}= -2l_1+2l_2
%\end{align}
%and this follows a $\chi^2$ distribution with $p_1-p_2$ degrees of freedom. The calculation
%\begin{align}
%P(t_{\chi^2_{p_1-p_2}>LRT} ).
%\end{align}
%gives us a two-sided p-value to fit with the two-sided alternative hypothesis.
%
% 
% \subsection{Testing whether one variance component is zero}
%  Hypothesis testing for one variance component is easy but not what most people expect.
%  In this setup, the two models have the exact same fixed effects. The only difference is the larger model has one more random effect $\nu_{t_1}$. This means we want to test whether $\nu_{t_1}=0$. A variance component must be nonnegative, meaning the alternative hypothesis is that $\nu_{t_1}>0$. In other words, the hypotheses are:
% \begin{align}
% H_0: \nu_{t_1}=0\\
% H_1: \nu_{t_1}>0.
% \end{align}
% Note the alternative hypothesis is one-sided. This means we need to calculate a one-sided p-value. Let $t_{LRT}=2l_2-2l_1$. If we naively follow the p-value calculation of
% \begin{align}
%P( \chi^2_{1}>t_{LRT})
%\end{align}
% we will end up calculating a two-sided p-value. To fix this, we cut this p-value in half. In other words, use the standard normal distribution $Z$ and think of calculating the one-sided p-value this way:
% \begin{align}
% P( Z>\sqrt{|  t_{LRT} |})
% \end{align}
% 
% However, this is not obvious; I never would have thought about having a test statistic of
% \begin{align}
% \sqrt{2 | l_2-l_1   |   }.
% \end{align}
% 
% \subsection{Testing whether multiple variance components are zero}
% In this case, the fixed effects are identical between the larger and nested model. The only difference is that the larger model has more than one additional variance components. This gets a lot more complicated (for example, there is no clear way to count parameters so calculating the degrees of freedom is out the window). Luckily, someone's worked out the details already. Charlie suggested a few papers to look at. I'll read about and add this a bit later.  Charlie is hoping that we'll have a mixture of $\chi^2$ distributions, such as
% \begin{align}
% \dfrac{1}{2}P (\chi^2_1> t_{LRT}) +\dfrac{1}{4}P (\chi^2_2> t_{LRT}).
% \end{align}
% The reason this is complicated is because the variance components are restricted to be nonnegative. This means that the likelihood is only defined for nonnegative variance components. Then differentiating the likelihood at $0$ becomes tricky because that's the boundary.
%
% \subsection{Testing whether one or more fixed effects is zero and one or more variance components are zero}\label{sec:combotest}
% This seems very complicated. I don't know if it's any more complicated than the previous case of testing multiple variance components. I'll have to think about it later when I have time.
% 
% \subsection{Determining the hypothesis test}
% To follow convention, this command will be called ``anova'' and the two arguments will be the two models to be compared.  The command would look like
%\begin{verbatim}
%anova(mod1,mod2)
%\end{verbatim} 
%
% R will first need to figure out if the fixed effects are different and, if they are, which model is the larger model.
%\begin{verbatim}
%if(length(coef(mod1))>length(coef(mod2)))
%      {bigmod<-mod1;  smallmod <-mod2} 
%if(length(coef(mod1))<length(coef(mod2)))
%      {bigmod<-mod2; smallmod <-mod1}
%\end{verbatim}
%Next, if there is a difference in the fixed effects, I'm going to make sure the fixed effects of the small model are nested in those of the big model. Otherwise, produce an error. Then, continue with the testing. Note: this check for nesting isn't perfect, but no other anova checks which model is bigger and whether they're nested.
%\begin{verbatim}
%if(big mod is defined) {
%      pnames1<-names(coef(bigmod))
%      pnames2<-names(coef(smallmod))
%      if(sum(pnames2 %in% pnames1) != length(pnames2)) {
%            stop("The models you provided are not nested.")}
%
%     if(the variance components differ between the two models){
%             check big model has more variance components (ow: error)
%             check variance components of the small model are nested (ow: error)
%             calculate p-value according to section 3.3.4. return it.
%      }
%      if(the variance components do not differ between the two models){
%            calculate p-value according to 3.3.1. return it.
%      }
%}
%\end{verbatim}
%If and only if we've gotten to the end of this chunk of code without returning anything, bigmod has not been defined, meaning the fixed effects are same. Therefore, we now need to figure out whether the variance components differ by one or by more than one. In order to compare the number of random effects, we need to figure out how to get the number of variance components from each model. I  don't yet have a solid grasp of how formula works its magic, but I have the impression that I can count the number of model matrices returned in order to figure out the number of variance components because there is a model matrix for each one.
%
%\begin{verbatim}
%T1<- number of variance components for mod1
%T2<- number of variance components for mod2
%if(T1>T2){call mod1 the bigmod and mod2 the smallmod}
%if(T1<T2){call mod2 the bigmod and mod1 the smallmod}
%if(bigmod is defined){
%     check  small model is nested in the big model (ow: error)
%     (maybe do this by looking at the call?)
%     if(T1==T2+1 || T2==T1+1) 
%           {calculate and return p-value according to sec 3.3.2}     
%     calculate and return p-value according to section 3.3.3
%     }
%if still haven't returned anything, produce error.     
%\end{verbatim}	
%
%Why the last error? If we have gotten to this point in the code with nothing returned, it ends up that the number of variance components in each model are equal, meaning the user has made some kind of mistake. Either the user provided two identical models or they provided non-nested models (e.g. they have the same number of variance components, but different components in each model). Either way, we can't help them.
%
%
%
%
%
%This command would return something that first reminds the user what predictors are in each model, then has a table. Each model would have one row of the table. The columns would contain the model name,  the log likelihood for each model, the number of parameters in the model. Then the next columns would have one entry each: the calculated difference between the log likelihoods, the calculated difference between the number of parameters, and the p-value of the test. 

%\section{AIC}
%Typing 
%\begin{verbatim}
%AIC(mod)
%\end{verbatim} 
%will return the AIC of the model. To calculate AIC, I'll need the total number of parameters ($p+T$)  and the Monte Carlo log likelihood evaluated at the MCMLEs $l$. Then the AIC is
%\begin{align}
%2(p+T) - 2 l 
%\end{align} 


\section{Confidence intervals}
The user can implement this command to create confidence intervals after  fitting a model $mod$ using the main function.  This is an S3 generic.The command would look like
\begin{verbatim}
confint(mod,parm,level=.95)
\end{verbatim}
The only required argument would be the fitted model (the first argument). The third argument (the confidence level) has a default of $.95$.

If the second argument is omitted, confidence intervals would be created for all of the parameters. There are two options to calculate confidence intervals for a subset of the parameters. The user can provide either  the names of the coefficients or a vector with length equal to the number of parameters (entries being 1 if they would like that intervals for that parameter and 0 otherwise).  The code to do this is taken from ``confint.lm'' and is:
\begin{verbatim}
function (object, parm, level = 0.95, ...) 
{
    cf <- coef(object)
    pnames <- names(cf)
    if (missing(parm)) 
        parm <- pnames
    else if (is.numeric(parm)) 
        parm <- pnames[parm]
        stopifnot(parm %in% pnames)
        
        rest of the code to actually do the confidence intervals goes here
        }
\end{verbatim}
What this says: write down the coefficient names of the object we're given. If parm is missing, we're going to assume they want to create intervals for all the parameters.  If parm is numeric (a vector of 0s and 1s), use that to select the parameter names we want and call that selection parm.  Finally, stop the whole process if parm (the parameter names we want intervals for) don't appear in the model.

The form for a confidence interval is the point estimate plus or minus the standard error of the point estimate times some cutoff.  The point estimate and the standard error are from the summary of the model. The reference distribution used for the cutoff is the standard normal. This will produce an asymptotic confidence interval.

The output would either be a vector of length 2 (if creating confidence interval for only one parameter) or a matrix with 2 columns (one for the lower bound of the confidence interval, one for the upper bond). The column names will be ``$(100-level)/2$'' and ``$50+level/2 $.'' (in the default case, this is``$2.5\%$'' and ``$97.5\%$'').

The only potential hangup is when creating confidence intervals for the variance components $\nu_t, t=1,...,T$.  We know that $\nu_t >0$ for all $t=1,...,T$. We'll find this boundary again makes things complicated. I don't yet know how to deal with this.  

To illustrate the problem, consider the scenario that the margin of error for $\nu_t$ is greater than the estimate of $\nu_t$ itself. It wouldn't make sense to produce a confidence interval with a negative lower bound. It could make sense to truncate the interval so that the lower bound starts at 0 and the upper bound remains untouched.  

\section{New Additions (Summer 2016)}
\subsection{New Family: Binomial}
One year later, I'm adding binomial. For Binomial, the calculations change only in the cumulant function. They rely on one more number: the number of trials. The inputted response is usually a vector, but for binomial (with the exception of Bernoulli) we need 2 vectors cbinded (cbound?) together. The first column is the number of successes and the second is the number of failures. Summing across the row gives us the number of trials. I call this $ntrials$ from here on out.

The cumulant function and its first two derivatives with respect to the canonical parameter $\eta_i$ are below. 
\begin{align}
c(\eta_i) &=ntrials_i \, \log(1+e^{\eta_i}) =
  \begin{cases}
   ntrials_i \, \log(1+e^{\eta_i}) & \text{if } \eta_i\leq 0,\\
   ntrials_i \,\left(  \eta_i+\log(e^{-\eta_i}+1) \right) & \text{if } \eta_i >0,
  \end{cases}\\
c'(\eta_i)&=\dfrac{ e^{{\eta_{i}}}}{ 1+e^{{\eta_{i}}}} = \dfrac{ntrials_i}{1+e^{-\eta_i}}\\
c''(\eta_i)&=   \dfrac{e^{{\eta_{i}}}}{  1+ e^{{\eta_{i}}} }  - \dfrac{e^{2{\eta_{i}}}}{    (  1+ e^{{\eta_{i}}})^2}  = \dfrac{ntrials_i}{1+e^{-\eta_i}}\cdot \dfrac{1}{1+e^{\eta_i}}
\end{align}

I will implement this change in steps. Game plan:
\begin{enumerate}
\item Without adding binomial yet, add the ntrials argument to all levels of code so that ntrials can be passed to the cumulant functions and their derivatives. At the top, I will have a variable to set ntrials = 1 (for now). As I do each layer, I will make sure I get all the same answers from my salamanders problem (or some other bernoulli problem).
\item Write the binomial cumulant in R. Do some checks with  both $\eta$ positive and negative.
\begin{enumerate}
 \item Check to make sure this calculation is identical to that produced by bernoulli for some parameter value $\eta_i$ and for $ntrials=1$. 
\item Check  the sum of Bernoulli cumulants equals the binomial cumulant (for ntrials $>1$). 
\item Check finite differences. 
\item Check the three calculations for $ntrials=5$ (or some number other than 1) compared to my by-hand calculation. 
\end{enumerate}
\item Code it in C and check that the calculation is identical to that produced in R for $ntrials=1$ and for $ntrials \neq 1$.
\item I will go to the top-most layer, right where the model matrices are being made. The new response vector y will be the first column of the user-provided response. The sum of the two entries in the row will create the vector ntrials. I will check that the salamanders problem gives the same results (when specified as cbind(Mate, 1-Mate)).

\item I will add checks for the model matrix stage. 
\begin{enumerate}
\item  If  the length of the dimension of the response is 2 dimensions (meaning we have both rows and columns), then the family must be binomial.glmm.
\item If the family is binomial.glmm, then the response must have 2 columns
\end{enumerate}
\item I want to allow the response for bernoulli to be a single vector  (rather than two cbinded vectors). Therefore, if the family is binomial.glmm and the length of the dimension of the response is 1 (meaning we just have a vector), then create the ntrials vector to be a vector of 1s. Test this with salamanders or some other problem to make sure it gives the same results.
\item Change PQL function to do binomial.
\end{enumerate}




These changes will need to be done in both C and R. The checks will be in R. I'll need to add  documentation for binomial. Bernoulli will be retained.


\subsection{MCSE}
Let  $\gamma_1$ be defined as
\begin{align}
  \gamma_1 = \int  f_\theta(u,y) \; du =  f_\theta(y). \label{eq:gamma1} 
\end{align}
My thesis shows the Monte Carlo error of the gradient of the log likelihood at the MLE $\hat{\theta}$ is 
\begin{align}
V= \dfrac{1}{\gamma_1^2}  \int \left[  \nabla \log f_{\hat{\theta}}(u,y)  \right] \left[  \nabla \log f_{\hat{\theta}}(u,y)  \right]^T \dfrac{f_{\hat{\theta}}(u,y)^2}{\tilde{f}(u)} \, du.
\end{align}



We can estimate this with the MCMLE $\hat{\theta}_m$:
\begin{align}
\hat{V}= \dfrac{ \dfrac{1}{m}  \sum_{k=1}^m \left( \left[\nabla \log f_{\hat{\theta}_m}(u_k,y)  \right] \left[\nabla \log f_{\hat{\theta}_m}(u_k,y)  \right]^T  \dfrac{ f_{\hat{\theta}_m}(u_k,y)^2   }{\tilde{f}(u_k)}  \right) }
{  \left( \dfrac{1}{m} \sum_{k=1}^m  \dfrac{ f_{\hat{\theta}_m}(u_k,y)   }{\tilde{f}(u_k)} \right)^2 }_.
\end{align}
%\begin{align}
%\hat{V}= \dfrac{   \sum_{k=1}^m \left( \left[ \nabla \log f_{\hat{\theta}_m} (u_k,y)  - \nabla l_m({\hat{\theta}_m}|y)   \right] \left[ \nabla \log f_{\hat{\theta}_m}(u_k,y)  -\nabla l_m({\hat{\theta}_m}|y)  \right]^T  \dfrac{ f_{\hat{\theta}_m}(u_k,y)   }{\tilde{f}(u_k)}  \right) }{\sum_{k=1}^m  \dfrac{ f_{\hat{\theta}_m}(u_k,y)   }{\tilde{f}(u_k)}}_.
%\end{align}

Let U be the Hessian of the likelihood evaluated at the MLE. This is estimated by
\begin{align}
\hat{U} = \nabla^2 l_m (\hat{\theta}_m |y).
\end{align}
In other words, $\hat{U}$ is the  MCLA Hessian evaluated at the MCMLE. 

Then the MCSE of $\hat{\theta}_m$ is the square root of the diagonal elements of
\begin{align} 
\dfrac{1}{m} \, \hat{U}^{-1} \, \hat{V} \, \hat{U}^{-1}
\end{align}
under the regularity conditions listed in \citet{geyer:1994}.

The breakdown for programming MCSE is:
\begin{enumerate}
\item Get $\hat{U}$ from output's \texttt{likelihood.hessian}
\item Stably invert $\hat{U}$
\item Calculate $\hat{V}$
\item Finish calculation (result will be a vector of MCSE with length equal to number of parameters)
\item Output table
\end{enumerate}
The output table will have a row for each parameter. The columns will be estimate (MCMLE), SE, MCSE. This will allow for easy comparison between SE and MCSE so that users can ensure their MCSE is small enough compared to the SE.

Note on calculating $\hat{U}$:the likelihood's Hessian is calculated for every call of objfunc. We can get this from the output's \texttt{likelihood.hessian}.

To do steps:
\begin{enumerate}
\item Remember how to do qr decomp (stably invert the matrix with qr.solve)
\item Calculate $\hat{V}$ and the rest of the MCSE calculation in R
\item Change the  $\hat{V}$ calculation to C
\end{enumerate}


\subsubsection{Cholesky Decomposition for Matrix Inversion}
Cholesky decomposition takes a symmetric, positive definite matrix and decomposes it into $LL'$, where $L$ is a lower triangular matrix and $L'$ is the transpose of L (so it's upper triangular). Triangular matrices are easy to solve using \texttt{backsolve} in R. For example, if you have the hessian \texttt{hess} and want to get the variance covariance matrix, then do 
\begin{verbatim}
cho <- chol(hess)
uinv <- backsolve(cho, diag(nrow(cho))
vcov <- uinv %*% t(uinv)
\end{verbatim}


\subsection{Revisit Summary Table}
I just realized I have \texttt{solve()} for calculating the variance-covariance matrix. I need to do a QR decomposition or something more stable.

%
%\section{Prediction intervals}
%This is another S3 generic. The user can implement this command to create prediction intervals after  fitting a model $mod$ using the main function. Still learning about these. I currently know almost nothing. What I do know is that I will need the Monte Carlo log likelihood along with its first and second derivatives. I'll get to practice my first-year-theory delta method skills.
%
%Charlie says that neither predict.glm nor predict.glmmPQL do prediction intervals. The former produces parameter estimates with standard errors. The latter does predictions but does not produce a standard error, so no interval can be formed.
%
%One wrong thing to do: once you find the MCMLEs, you simply plug those into the distribution for $U|Y$ and pretend those parameter estimates have no variability and are  correct. Then the random effect has a normal distribution and it'd be easy to find quantiles. The only problem is this is wrong.
%
%We are hoping to find a way to use Fisher information for the Monte Carlo MLE. Hopefully we can do something more similar to the usual asymptotics.
%
%A last resort is a parametric bootstrap. I haven't spent much time thinking about this, but I think the way this involves plugging parameter estimates  into $\log f_\theta(y|u)$ and $\log f_\theta(u)$. Then you repeat: draw a value of $u$, plug that into $\log f_\theta(y|u)$, draw a value of $y$. I say this is a last resort  because having two Monte Carlo calculations will take a long time. 




\bibliographystyle{apalike}
\bibliography{brref}

\appendix
\section{MCLA calculations}
Let $u_k,k=1,\ldots,m$ be a sample from $\tilde{f}(u_k)$. The Monte Carlo log likelihood approximation is
\begin{align}
l_{m}(\theta) &=\log \dfrac{1}{m} \sum_{k=1}^mf_\theta(y|u_k)  \dfrac{ f_\theta(u_k)   }{\tilde{f}(u_k)}\\
&= \log \dfrac{1}{m} \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}.
\end{align}
The gradient vector and Hessian matrix calculations depend on whether $\tilde{f}(u_k)$ contains $\theta$. The calculations in \ref{sec:calcs} are for $\tilde{f}(u_k)$ containing $\theta$ and the calculations in \ref{sec:calcsindep} are for $\tilde{f}(u_k)$ not containing $\theta$.

\subsection{MCLA derivatives when $\tilde{f}$ independent of $\theta$}\label{sec:calcsindep}

When $\tilde{f}$ is independent of $\theta$, the gradient vector of the MCLA with respect to $\theta$ is
\begin{align}
\nabla l_m(\theta)= \dfrac{\sum_{k=1}^m    \left( \nabla \log f_\theta(u_k,y)  \dfrac{f_\theta(u_k,y)   }{\tilde{f}(u_k)}\right) }{\sum_{k=1}^m \left( \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) } 
\end{align}
and the Hessian matrix of the MCLA is
\begin{align}
\nabla^2 l_m(\theta)&= \dfrac{\sum_{k=1}^m    \left( \nabla^2 \log f_\theta(u_k,y)   \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right)}{\sum_{k=1}^m \left( \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) }- \nabla l_m(\theta) (\nabla l_m(\theta) )'  \\
&+\dfrac{\sum_{k=1}^m    \left( \nabla \log f_\theta(u_k,y) (\nabla \log f_\theta(u_k,y))'   \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right)}{\sum_{k=1}^m \left( \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) }.
\end{align}
To reduce the risk of catastrophic cancellation, we can combine the last two terms of the Hessian:
\begin{align}
\nabla^2 l_m(\theta)&= \dfrac{   \sum_{k=1}^m  \nabla^2 \log f_\theta(y,u_k)       \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)   - \nabla l_m(\theta)   \right] \left[ \nabla \log f_\theta(y,u_k)  - \nabla l_m(\theta)  \right]'  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}   }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}
\end{align}
We are able to combine the last two terms because $\nabla l_m(\theta)$ is a weighted mean of $\nabla \log f_\theta (y,u_k)- \nabla \log \tilde{f}(u_k)$. Letting
\begin{align}
Z&=\nabla \log f_\theta (y,u_k)
\end{align}
 we can use the following equality:
\begin{align}
E(ZZ')-E(Z)E(Z)' = \left[ E(Z-EZ)  \right]\left[ E(Z-EZ)  \right]'.
\end{align}




\subsection{MCLA derivatives when $\tilde{f}$ depends on $\theta$} \label{sec:calcs}
This section is still under construction.

When $\tilde{f}$ contains $\theta$, the gradient of the MCLA is
\begin{align}
\nabla l_m(\theta) &= \dfrac{ \nabla \left[ \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}  \right] }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \\
&= \dfrac{   \sum_{k=1}^m  \dfrac{\nabla f_\theta(y,u_k)   }{\tilde{f}(u_k)} - \dfrac{ f_\theta(y,u_k)  \nabla \tilde{f}(u_k) }{\left(\tilde{f}(u_k) \right)^2 } }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \\
&= \dfrac{   \sum_{k=1}^m  \dfrac{\nabla f_\theta(y,u_k)}{f_\theta(y,u_k)} \dfrac{f_\theta(y,u_k)}{\tilde{f}(u_k)} -
 \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)} \dfrac{ \nabla \tilde{f}(u_k) }{\tilde{f}(u_k) } }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \\
&= \dfrac{   \sum_{k=1}^m  \nabla \log f_\theta(y,u_k) \dfrac{f_\theta(y,u_k)}{\tilde{f}(u_k)} -
 \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  \nabla \log \tilde{f}(u_k)  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \\
&= \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} 
\end{align}

Then the Hessian of the MCLA is
\begin{align}
\nabla^2 l_m (\theta) &= \nabla \left[ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}} \right] \\
&= \dfrac{   \sum_{k=1}^m \left[ \nabla^2 \log f_\theta(y,u_k)  -
   \nabla^2 \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]  \nabla \left[ \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)} \right]  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&-\left[\dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{ \left( \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)} \right)^2} \right] \nabla \left[ \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)} \right]\\
&= \dfrac{   \sum_{k=1}^m \left[ \nabla^2 \log f_\theta(y,u_k)  -
   \nabla^2 \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right] \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]'  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}   }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&-\left[\dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -  \nabla \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{ \left( \sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)} \right)^2} \right] \left[  \nabla \log f_\theta(y,u_k)  -  \nabla \log \tilde{f}(u_k)     \right]'\\
&= \dfrac{   \sum_{k=1}^m \left[ \nabla^2 \log f_\theta(y,u_k)  -
   \nabla^2 \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right] \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k)   \right]'  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}   }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&- \left[ \nabla l_m(\theta)  \right] \left[ \nabla l_m(\theta)  \right]'
\end{align}
To reduce the risk of catastrophic cancellation, we can combine the last two terms of the Hessian:
\begin{align}
\nabla^2 l_m(\theta)&= \dfrac{   \sum_{k=1}^m \left[ \nabla^2 \log f_\theta(y,u_k)  -
   \nabla^2 \log \tilde{f}(u_k)   \right]  \dfrac{ f_\theta(y,u_k)}{\tilde{f}(u_k)}  }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}\\
&+ \dfrac{   \sum_{k=1}^m \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k) - \nabla l_m(\theta)   \right] \left[ \nabla \log f_\theta(y,u_k)  -
   \nabla \log \tilde{f}(u_k) -\nabla l_m(\theta)  \right]'  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}   }{\sum_{k=1}^m  \dfrac{ f_\theta(y,u_k)   }{\tilde{f}(u_k)}}
\end{align}
We are able to combine the last two terms because $\nabla l_m(\theta)$ is a weighted mean of $\nabla \log f_\theta (y,u_k)- \nabla \log \tilde{f}(u_k)$. Letting
\begin{align}
Z&=\nabla \log f_\theta (y,u_k)- \nabla \log \tilde{f}(u_k)
\end{align}
 we can use the following equality:
\begin{align}
E(ZZ')-E(Z)E(Z)' = \left[ E(Z-EZ)  \right]\left[ E(Z-EZ)  \right]'.
\end{align}






\section{Central Limit Theorem for MCLA}\label{sec:CLT}
In this section, we focus on the gradient of the MCLA because the trust criterion for finding the maximum is based on the gradient when $\tilde{f}(u)$ is independent of $\theta$. 
Define
\begin{align}
\gamma_1&= \int f_\theta(u,y) du\\
\gamma_2 &=\int \tilde{f}(u)du.
\end{align}
Recall the calculation for the MCLA gradient originally stated in  \eqref{eq:MCLAgradient}:
\begin{align}
\nabla l_m(\theta)&= \dfrac{\sum_{k=1}^m     \nabla \log f_\theta(u_k,y)  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }{\sum_{k=1}^m \left( \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) }
\end{align}

 Note that both the numerator and denominator are sample means. By the law of large numbers, the numerators and denominator  each converge to their true means. That is,
\begin{align}
\dfrac{1}{m}\sum_{k=1}^m \nabla \log f_\theta (u_k,y)  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} &\rightarrow  E_{\tilde{f}} \left[ \nabla \log f_\theta (u,y)  \dfrac{f_\theta(u,y)}{\tilde{f}(u)}  \right] \\
&= \int \nabla \log f_\theta (u,y)  \dfrac{f_\theta(u,y)}{\tilde{f}(u)} \dfrac{\tilde{f}(u)}{\gamma_2} du\\
&= \dfrac{\gamma_1}{\gamma_2} \int \nabla \log f_\theta (u,y)  \dfrac{f_\theta(u,y)}{\gamma_1}  du \\
&=\dfrac{\gamma_1}{\gamma_2} E_f \left[ \nabla \log f_\theta(u,y)  \right]
\end{align}
%and
%\begin{align}
%\dfrac{1}{m}\sum_{k=1}^m \nabla \log \tilde{f}(u_k)  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} &\rightarrow  E_{\tilde{f}} \left[ \nabla \log \tilde{f}(u)  \dfrac{f_\theta(u,y)}{\tilde{f}(u)}  \right] \\
%&= \int \nabla \log \tilde{f}(u)  \dfrac{f_\theta(u,y)}{\tilde{f}(u)} \dfrac{\tilde{f}(u)}{\gamma_2} du\\
%&= \dfrac{\gamma_1}{\gamma_2} \int \nabla \log \tilde{f}(u)  \dfrac{f_\theta(u,y)}{\gamma_1}  du \\
%&=\dfrac{\gamma_1}{\gamma_2} E_f \left[ \nabla \log \tilde{f}(u)  \right]
%\end{align}
and
\begin{align}
\dfrac{1}{m}\sum_{k=1}^m \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} &\rightarrow E_{\tilde{f}} \left[ \dfrac{f_\theta(u,y)}{\tilde{f}(u)}  \right] \\
&= \int \dfrac{f_\theta(u,y)}{\tilde{f}(u)} \dfrac{\tilde{f}(u)}{\gamma_2} du\\
&= \dfrac{\gamma_1}{\gamma_2} \int \dfrac{ f_\theta(u,y)}{\gamma_2} du \\
&= \dfrac{\gamma_1}{\gamma_2}
\end{align}
Then, by Slutsky's theorem,
\begin{align}
\nabla l_m(\theta)&\rightarrow \dfrac{  \dfrac{\gamma_1}{\gamma_2} E_f \left[ \nabla \log f_\theta(u,y)  \right] }{\dfrac{\gamma_1}{\gamma_2}}\\
&=  E_f \left[ \nabla \log f_\theta(u,y) \right] \\
&=\nabla l(\theta)
\end{align}
In addition to a law of large numbers, we would like a Central Limit Theorem for $\nabla l_m(\theta)$. In other words, the quantity
\begin{align}
&\sqrt{m} \left[  \dfrac{\dfrac{1}{m}\sum_{k=1}^m    \nabla \log f_\theta(u_k,y)   \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }{\dfrac{1}{m} \sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}  }- \nabla l(\theta)  \right] \\
&=
\dfrac{\dfrac{1}{\sqrt{m}}\sum_{k=1}^m     \nabla \log f_\theta(u_k,y)   \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }{\dfrac{1}{m} \sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }- \sqrt{m}\; \nabla  l(\theta)  \\
&=\dfrac{\dfrac{1}{\sqrt{m}}\sum_{k=1}^m     \nabla \log f_\theta(u_k,y)   \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} 
 -\nabla l(\theta) \dfrac{1}{\sqrt{m}}\sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }{\dfrac{1}{m} \sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} }   
\end{align}
will have a normal distribution if and only if the variances of
\begin{align}
\sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}
\end{align}
and
\begin{align}
\sum_{k=1}^m     \nabla \log f_\theta(u_k,y)\dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} 
\end{align}
are finite.
First, we want to show that
\begin{align}
\sum_{k=1}^m \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}
\end{align}
has finite variance. This is true if and only if 
\begin{align}
\int \dfrac{f_\theta(u,y)^2}{\tilde{f}(u)} du < \infty.
\end{align}
We see:
\begin{align}
\int \dfrac{f_\theta(u,y)^2}{\tilde{f}(u)} du &=\int \dfrac{f_\theta(y|u)^2 f_\theta(u)^2}{\tilde{f}(u)} du\\
&\leq \int \dfrac{f_\theta(y|u)^2 f_\theta(u)^2}{p_1 \grave{f}(u|0,D^*)} du\\
&\propto \dfrac{1}{p_1} \int \dfrac{\exp(-u'D^{-1}u) \, f_\theta(y|u)^2  }{\left[ 1+u'D^*u/\zeta  \right]^{-(\zeta+q)/2}} du
\end{align}
Since
\begin{align}
f_\theta(y|u)^2=\left( e^{y'\eta-c(\eta)} \right)^2 = e^{2y'\eta-2c(\eta)} ,
\end{align}
\begin{align}
\int \dfrac{f_\theta(u,y)^2}{\tilde{f}(u)} du &\propto \dfrac{1}{p_1} \int \dfrac{\exp(-u'D^{-1}u) \, \exp(2y'\eta)  }{\left[ 1+u'D^*u/\zeta  \right]^{-(\zeta+q)/2} \,  \exp(2c(\eta))  } du\\
&=\dfrac{1}{p_1} \int \dfrac{ \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} \, \exp(2y'\eta)  }{\exp(u'D^{-1}u)   \,  \exp(2c(\eta))  } du \\
&=\dfrac{1}{p_1} \int \dfrac{ \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} \, \exp(2y'Zu)  }{\exp(u'D^{-1}u)   \,  \exp(2c(\eta))  } du 
\end{align}
When $y|u\sim$ Bernoulli, then
\begin{align}
c(\eta)=\log(1+e^\eta) \Rightarrow \exp(2c(\eta))&= \exp( 2 \log(1+e^\eta)  )\\
&= ( \exp( \log(1+e^\eta)  )  )^2\\
&= (1+e^\eta) ^2
\end{align}
and
\begin{align}
\int \dfrac{f_\theta(u,y)^2}{\tilde{f}(u)} du &\propto \dfrac{1}{p_1} \int \dfrac{ \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} \, \exp(2y'Zu)  }{\exp(u'D^{-1}u)   \,  (1+e^\eta) ^2  } du,
\end{align}
which converges since $\exp(u'D^{-1}u$ grows faster than any term in the numerator of the integrand.

On the other hand, when $y|u\sim$ Poisson, then 
\begin{align}
c(\eta)=\exp(\eta) \Rightarrow \exp(2 c(\eta)) = (\exp(\exp(\eta)))^2
\end{align}
and
\begin{align}
\int \dfrac{f_\theta(u,y)^2}{\tilde{f}(u)} du &\propto \dfrac{1}{p_1} \int \dfrac{ \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} \, \exp(2y'Zu)  }{\exp(u'D^{-1}u)   \,   (\exp(\exp(\eta)))^2  } du,
\end{align}
which converges since  $(\exp(\exp(\eta)))^2$ grows faster than any term in the numerator of the integrand.  

Therefore, whether the data are Bernoulli or Poisson distributed, the importance sampling weights 
\begin{align}
\sum_{k=1}^m  \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}
\end{align}
have finite variance.

Now, we want to show that
\begin{align}
\sum_{k=1}^m \nabla \log f_\theta (u_k,y) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} =\sum_{k=1}^m \nabla \log f_\theta (y|u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} +\sum_{k=1}^m \nabla \log f_\theta (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} 
\end{align}
 has finite variance. We need to show the following are finite:
\begin{align}
\var \left( \sum_{k=1}^m \nabla \log f_\theta (y|u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}   \right) \\
\var \left(\sum_{k=1}^m \nabla \log f_\theta (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right)\\
\cov \left(\sum_{k=1}^m \nabla \log f_\theta (y|u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \, , \, \sum_{k=1}^m \nabla \log f_\theta (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right)_.
\end{align}

That is, we want to show the existence of the following:
\begin{align}
&\int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du \label{eq:first} \\
&\int \dfrac{ \left[ \nabla \log f_\theta (u) \right]^2 \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du \label{eq:second} \\
&\int \dfrac{  \nabla \log f_\theta (u)  \nabla \log f_\theta (y|u)  \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du \label{eq:third} 
\end{align}

First, we start with \eqref{eq:first}:
\begin{align}
\int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du 
&=\int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{\tilde{f}(u)} du \\
&\leq \int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{p_1\grave{f}(u|0,D^*)} du \\
&\propto \int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(y|u)\right]^2 \exp(-u'D^{-1}u)}{p_1\left[ 1+u'D^*u/\zeta  \right]^{-(\zeta+q)/2}} du \\
&= \int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \left[ f_\theta(y|u)\right]^2 \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \exp(u'D^{-1}u)} du \\
&= \int \dfrac{ \left[ \nabla \log f_\theta (y|u) \right]^2 \, \exp(2y'(X \beta+Zu)) \,  \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{ p_1 \, \exp(2 c(\eta)) \,\exp(u'D^{-1}u)} du \label{eq:soon}
\end{align}
By the Cauchy-Schwartz inequality, the integral in \eqref{eq:soon} exists if
\begin{align}
 & \int \dfrac{ \left[\dfrac{\partial}{\partial \beta} \log f_\theta (y|u) \right]^2 \, \exp(2y'(X \beta+Zu)) \,  \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{ p_1 \, \exp(2 c(\eta)) \,\exp(u'D^{-1}u)} du \nonumber \\
&\propto  \int \dfrac{ \left[X' c'(\eta) \right]^2 \, \exp(2y'(X \beta+Zu)) \,  \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{ p_1 \, \exp(2 c(\eta)) \,\exp(u'D^{-1}u)} du \label{eq:soon2}
\end{align}
exists, where $c'(\eta)$ is a vector with components $c'(\eta_i)$.  This will converge as long as no term in the numerator of the integrand grows more quickly than $\exp(u'D^{-1}u$. The second and third terms grow less quickly than $\exp(u'D^{-1}u$, so $\left[ c'(\eta)\right]^2$ is the only term to check.  If $y|u \sim$ Bernoulli, then 
\begin{align}
c'(\eta_i)= \dfrac{\exp(\eta_i)}{1+\exp(\eta_i)},
\end{align}
which does not grow more quickly than $\exp(u'D^{-1}u)$. If $y|u \sim$ Poisson, then
\begin{align}
c'(\eta_i)= e^{\eta_i},
\end{align}
which does not grow more quickly than $\exp(u'D^{-1}u)$. Therefore, \eqref{eq:first} exists and is finite, and 
\begin{align}
\var \left( \sum_{k=1}^m \nabla \log f_\theta (y|u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) < \infty.
\end{align}
is finite.

Next, we move on to showing the existence of \eqref{eq:second}.
\begin{align}
\int \dfrac{ \left[ \nabla \log f_\theta (u) \right]^2 \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du &= 
\int \dfrac{ \left[ \nabla \log f_\theta (u) \right]^2 \left[ f_\theta(y|u)\right]^2  \left[ f_\theta(u)\right]^2}{\tilde{f}(u)} du\\
&\leq \int \dfrac{ \left[ \nabla \log f_\theta (u) \right]^2 \left[ f_\theta(y|u)\right]^2  \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du
\end{align}
By the Cauchy-Schwartz inequality, the above integral exists as long as  
\begin{align}
\int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t} \log f_\theta (u) \right]^2 \left[ f_\theta(y|u)\right]^2  \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du < \infty
\end{align}
for every $t=1,\ldots, T$. We see
\begin{align}
&\int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t} \log f_\theta (u) \right]^2 \left[ f_\theta(y|u)\right]^2  \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du \\
&= \int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t} \log f_\theta (u) \right]^2 \exp(2y'(X\beta+Zu)   )  \left[ f_\theta(u)\right]^2}{p_1 \exp(2c(\eta))    \grave{f}(u|0,D^*)} du \\
&\propto \int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t} \log f_\theta (u) \right]^2 \exp(2y'(X\beta+Zu)   ) \,   \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \exp(2c(\eta)) \, \exp(u'D^{-1}u)  } du\\
&= \int \dfrac{ \left[  -\dfrac{q_t}{2 \nu_t} + \dfrac{u_t'u_t}{2 \nu_t^2}   \right]^2 \exp(2y'(X\beta+Zu)   ) \,   \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \exp(2c(\eta)) \, \exp(u'D^{-1}u)  } du
\end{align}
Because every term in the numerator of the integrand grows less quickly than $\exp(u'D^{-1}u)$, the integral exists.  Therefore, \eqref{eq:second} exists and 
\begin{align}
\var \left(\sum_{k=1}^m \nabla \log f_\theta (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) < \infty.
\end{align}

Lastly, we want to show the existence of \eqref{eq:third}. We see
\begin{align}
\int \dfrac{  \nabla \log f_\theta (u)\,  \nabla \log f_\theta (y|u) \, \left[ f_\theta(u,y)\right]^2}{\tilde{f}(u)} du &=
\int \dfrac{  \nabla \log f_\theta (u) \, \nabla \log f_\theta (y|u) \, \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{\tilde{f}(u)} du \\
&\leq
\int \dfrac{  \nabla \log f_\theta (u) \,  \nabla \log f_\theta (y|u)\,  \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du \label{eq:this1}
\end{align}
Again, we use the Cauchy-Schwartz inequality to check the existence of \eqref{eq:this1}. If, for every $t=1,\ldots,T$,
\begin{align}
\int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t}  \log f_\theta (u) \right] \left[  \dfrac{\partial}{\partial \beta} \log f_\theta (y|u) \right]   \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du \label{eq:this1}
\end{align}
exists, then \eqref{eq:this1} also exists. Continuing, we see
\begin{align}
& \int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t}  \log f_\theta (u) \right] \left[  \dfrac{\partial}{\partial \beta} \log f_\theta (y|u) \right]   \left[ f_\theta(y|u)\right]^2 \left[ f_\theta(u)\right]^2}{p_1 \grave{f}(u|0,D^*)} du\\
 &\propto \int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t}  \log f_\theta (u) \right] \left[  \dfrac{\partial}{\partial \beta} \log f_\theta (y|u) \right]   \left[ f_\theta(y|u)\right]^2 }{p_1 \, \exp(u'D^{-1}u) \, \grave{f}(u|0,D^*)} du\\
 &\propto \int \dfrac{ \left[ \dfrac{\partial}{\partial \nu_t}  \log f_\theta (u) \right] \left[  \dfrac{\partial}{\partial \beta} \log f_\theta (y|u) \right]   \left[ f_\theta(y|u)\right]^2 \, \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \, \exp(u'D^{-1}u) } du\\
&\propto \int \dfrac{ \left[      -\dfrac{q_t}{2 \nu_t} + \dfrac{u_t'u_t}{2 \nu_t^2}        \right] \left[  X'(y-c'(\eta)) \right]   \left[ f_\theta(y|u)\right]^2 \, \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \, \exp(u'D^{-1}u) } du\\
&= \int \dfrac{ \left[      -\dfrac{q_t}{2 \nu_t} + \dfrac{u_t'u_t}{2 \nu_t^2}        \right] \left[  X'(y-c'(\eta)) \right]   \, \exp(2y'(X\beta+Zu)) \, \left[ 1+u'D^*u/\zeta  \right]^{(\zeta+q)/2} }{p_1 \, \exp(u'D^{-1}u) \, \exp(2 c(\eta)) } du. \label{eq:this2}
\end{align}
We have already shown that $-c'(\eta)$ does not grow more quickly than $\exp(u'D^{-1}u)$. The other terms in the numerator of the integrand shown in \eqref{eq:this2} also grow more slowly than $\exp(u'D^{-1}u)$. We conclude that \eqref{eq:this2} exists, which implies that \eqref{eq:third} exists and
\begin{align}
\cov \left(\sum_{k=1}^m \nabla \log f_\theta (y|u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \, , \, \sum_{k=1}^m \nabla \log f_\theta (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) < \infty.
\end{align}
This shows that
\begin{align}
\var \left( \sum_{k=1}^m     \nabla \log f_\theta(u_k,y)\dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)} \right) < \infty. 
\end{align}
Therefore, the gradient of the MCLA has finite variance and a central limit theorem.

%EDITED TO HERE.
%
% That is, we want to show the existence of
%\begin{align}
%\int \dfrac{N(u|0,D)^2 \left[\nabla \left(\log f_\theta(y|u)+\log f_\theta (u) \right) \right]^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du.
%\end{align}
%That is, we want to show the existence of
%\begin{align}
%\int &\dfrac{N(u|0,D)^2 \left[\nabla \left(\log f_\theta(y|u)+\log f_\theta (u) \right) \right]^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du \\
%&\leq \int  \dfrac{N(u|0,D)^2 \left[\nabla \left(\log f_\theta(y|u) + \log f_\theta (u) \right) \right]^2}{p_1 N(u|0,D)} du  \\
%&\leq  \int \dfrac{N(u|0,D)^2 \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u) \, \, \dfrac{\partial}{\partial \nu} \log f_\theta (u)  \right]^2}{p_1 N(u|0,D)} du  \\
%&\leq \dfrac{1}{p_1}  \int N(u|0,D) \left[ \dfrac{\partial}{\partial \beta}\log f_\theta(y|u) \, \, \dfrac{\partial}{\partial \nu} \log f_\theta (u)  \right]^2 du.
%\end{align}
%By the Cauchy-Schwartz inequality, 
%\begin{align}
%  &\int N(u|0,D) \left[ \dfrac{\partial}{\partial \beta}\log f_\theta(y|u) \, \, \dfrac{\partial}{\partial \nu} \log f_\theta (u)  \right]^2 du \\
%& \leq \left[  \int N(u|0,D) \left[  \dfrac{\partial}{\partial \nu} \log f_\theta (u)  \right]^2 du \right]^{1/2}  \left[  \int N(u|0,D) \left[ \dfrac{\partial}{\partial \beta}\log f_\theta(y|u)   \right]^2 du \right]^{1/2} .
%\end{align}
%Therefore, I need to show that each of these two integrals is finite.  I will start with the first of the two integrals: 
%\begin{align}
%\int N(u|0,D) \left[  \dfrac{\partial}{\partial \nu} \log f_\theta(u)  \right]^2 du. 
%\end{align}
%Recall the notation from section \ref{sec:Ddiag}.  If
%\begin{align}
%\int N(u_t|0,D_t) \left[  \dfrac{\partial}{\partial \nu_t} \log f_\theta(u)  \right]^2 du_t< \infty 
%\end{align}
%for every $t=1,...,T$, then
%\begin{align}
%\int N(u|0,D) \left[  \dfrac{\partial}{\partial \nu} \log f_\theta(u)  \right]^2 du < \infty
%\end{align}
%by Cauchy Schwartz.  Therefore, I will prove the integral is finite for the general case of $u_t$ and $\nu_t$. First, we recognize that our integral is actually an expectation of a function of a normal random variable:
%\begin{align}
%\int N(u_t|0,D_t) \left[  \dfrac{\partial}{\partial \nu_t} \log f_\theta(u)  \right]^2 du_t &=
%\int \dfrac{1}{\nu_t^{q_t/2}} e^{-u_t'u_t/(2 \nu_t)} \left[ -\dfrac{q_t}{2 \nu_t} + \dfrac{u_t'u_t}{2 \nu_t^2}  \right]^2 du_t .
%\end{align}
%Normal distributions have finite moments of all orders, so we know that\begin{align}
%\int N(u_t|0,D_t) \left[  \dfrac{\partial}{\partial \nu_t} \log f_\theta(u)  \right]^2 du_t < \infty.
%\end{align}
%
%
%
%
%Next, we want to show 
%\begin{align}
%\int N(u|0,D) \left[\dfrac{\partial}{\partial \beta} \log f_\theta(y|u)   \right]^2 du < \infty.
%\end{align}
% Recall that $\eta=X \beta +Z u$. Letting $\dfrac{\partial}{\partial \eta} c(\eta)=\mu$, we see 
%\begin{align}
%\int N(u|0,D) \left[\dfrac{\partial}{\partial \beta} \log f_\theta(y|u)   \right]^2 du &= \int N(u|0,D) \left[ (Y'- \mu') X X'(Y- \mu)   \right] du
%%&\propto  \int N(u|0,D) \mu' XX' Y du + \int N(u|0,D) \mu' X X' \mu du   .
%\end{align}
%We will consider this integral for the cases of $Y|u$ being a Bernoulli and a Poisson random variable. When $Y|u$ is Bernoulli, then $0 \leq \mu \leq 1.$ Therefore,
%\begin{align}
%\int N(u|0,D) \left[ (Y'- \mu') X X'(Y- \mu)   \right] du
%\end{align}
%exists and is finite.
%When $Y|u$ is a Poisson random variable, then for any  $\eta$, 
%\begin{align}
%\dfrac{\partial}{\partial \eta} c(\eta) = e^\eta.
%\end{align}
%Then
%\begin{align}
%\int N(u|0,D) \left[ (Y'- \mu') X X'(Y- \mu)   \right] du &\propto \int N(u|0,D)  e^{\eta} e^{\eta}    du \\
%&\propto \int N(u|0,D) e^{2Zu} du 
%\end{align}
%exists and is finite.
%
%Next, we want to show that 
%\begin{align}
%\sum_{k=1}^m \nabla \log \tilde{f} (u_k) \dfrac{f_\theta(u_k,y)}{\tilde{f}(u_k)}
%\end{align}
% has finite variance. That is, we want to show the existence of
%\begin{align}
%\int \dfrac{N(u|0,D)^2 \left[\nabla \log \tilde{f}u) \right]^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du.
%\end{align}
%We see that
%\begin{align}
%\int& \dfrac{N(u|0,D)^2 \left[\nabla \log \tilde{f}u) \right]^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du\\
%&\leq  \int \dfrac{N(u|0,D)^2 \left[\nabla \log \tilde{f}u) \right]^2}{p_1 N(u|0,D)} du\\
%&= \dfrac{1}{p_1} \int N(u|0,D) \left[\nabla \log \tilde{f}u) \right]^2 du\\
%&=\dfrac{1}{p_1} \int N(u|0,D) \left[  \dfrac{p_1 \dfrac{\partial}{\partial \nu} N(u|0,D)}{p_1 N(u|0,D)+p_2 N(u|u^*,D^*)+p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})}   \right]^2 du\\
%&\leq\dfrac{1}{p_1} \int N(u|0,D) \left[  \dfrac{p_1 \dfrac{\partial}{\partial \nu} N(u|0,D)}{p_1 N(u|0,D)}   \right]^2 du\\
%&=\dfrac{1}{p_1} \int N(u|0,D) \left[  \dfrac{ \dfrac{\partial}{\partial \nu} N(u|0,D)}{ N(u|0,D)}   \right]^2 du\\
%&= \dfrac{1}{p_1} \int \dfrac{\left[ \dfrac{\partial}{\partial \nu} N(u|0,D)  \right]^2}{N(u|0,D)} du \\
%\end{align}
%Now we use
%\begin{align}
%\dfrac{\partial}{\partial \nu} \log N(u|0,D) = \dfrac{\dfrac{\partial}{\partial \nu}  N(u|0,D)}{N(u|0,D)}
%\end{align}
%to see
%\begin{align}
% \dfrac{1}{p_1} \int \dfrac{\left[ \dfrac{\partial}{\partial \nu} N(u|0,D)  \right]^2}{N(u|0,D)} du &=\dfrac{1}{p_1} \int  N(u|0,D) \left[ \dfrac{\partial}{\partial \nu} \log N(u|0,D)  \right]^2 du \\
%&= \dfrac{1}{p_1} \int  N(u|0,D) \left[ \dfrac{\partial}{\partial \nu} \log N(u|0,D)  \right]^2 du 
%\end{align}
%which exists and is finite.
%
%
%
%Therefore, it is proven that 
%\begin{align}
%\int \dfrac{N(u|0,D)^2 \left[\nabla \left(\log f_\theta(y|u)+\log f_\theta (u) \right) \right]^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du 
%\end{align}
%exists and is finite. Therefore, since  the gradient of the MCLA has finite variance, we can conclude the gradient of the MCLA has a Central Limit Theorem.

%
%Next, we need to show that the Hessian of the MCLA has finite variance. Again, it is enough to show that
%\begin{align}
%\int \dfrac{N(u|0,D)^2 h(u)^2}{p_1 N(u|0,D)+p_2 N(u|u^*, D^*) + p_3 N(u|u^*, ((Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})} du < \infty
%\end{align}


%\subsection{Boundedness of importance weights}
%We want to make sure that the importance weights will be bounded when using this distribution. The importance weight for a vector $u_k$ is $\dfrac{f_\theta(y,u_k)}{\tilde{f}(u_k)}$. Since $f_\theta(y,u_k)< \infty$, 
%\begin{align}
%\dfrac{f_\theta(y,u_k)}{\tilde{f}(u_k)} < \infty \Longleftrightarrow \tilde{f}(u_k) >0.
%\end{align}
%
%We start with
%\begin{align}
% \tilde{f}(u_k) &= p_1  f(u_k \, | \, 0, D)+p_2  f(u_k \, | \, u^*, D^*)+p_3  f(u_k \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})\\
%&\geq p_1  f(u_k \, | \, 0, D) \\
%& \geq p_1 (2 \pi)^{-q/2} \; |(D^*)^{-1/2}|  \; e^{- u' (D^{*})^{-1}u}
%\end{align}
%In order to show that $\tilde{f}(u_k)>0$, we need to show that both $ |(D^*)^{-1/2}|$ and  $e^{- u' (D^{*})^{-1}u}$ are positive. 
%
% First, I will show that $ |(D^*)^{-1/2}|>0$.Since  $(D^*)^{-1/2}$ is invertible, its determinant must be nonzero.  Since $D^*$ is positive semi-definite, then $(D^*)^{-1/2}$ is also positive semi-definite. Thus the eigenvalues of $(D^*)^{-1/2}$ are neither zero nor negative; they must be positive. An eigendecomposition tells us $|(D^*)^{-1/2}|$ is the product of its eigenvalues. Therefore, $|(D^*)^{-1/2}| >0$. 
%
%Next, I will show that $e^{- u' (D^{*})^{-1}u}>0$.  Since $D^*$ is a covariance matrix, it is positive semi-definite.  By definition, for any vector $a_1$, 
%\begin{align}
%a_1' D^* a_1 \geq 0
%\end{align}
%For every vector $a_2$, there exists an $a_1$ such that $a_2=D^*a_1$. Then
%\begin{align}
%a_2' (D^*)^{-1} a_2 &= (D^* a_1)' (D^*)^{-1} D^* a_1\\
%&= a_1' D^* (D^*)^{-1} D^* a_1\\
%&= a_1' D^* a_1\\
%& \geq 0
%\end{align}
%Therefore,   $(D^*)^{-1}$ is also positive semi-definite. Then for any vector of random effects $u$,
%\begin{align}
% u' (D^*)^{-1} u \geq 0 
%\end{align}
%This means that
%\begin{align}   e^{- u' (D^{*})^{-1}u} >0
%\end{align}
%Therefore,
%\begin{align}
% \tilde{f}(u_k) &\geq p_1 (2 \pi)^{q/2} \; |(D^*)^{-1/2}|  \; e^{- u' (D^{*})^{-1}u} >0
%\end{align}
%




\end{document}
